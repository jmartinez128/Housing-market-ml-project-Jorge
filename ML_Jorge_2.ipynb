{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Changing the names of the Column names in the dataset to make it easier to understand\n",
    "column_mapping = {\n",
    "    # Location and Reference Information\n",
    "    'MapRefNo': 'mapping_reference_number',\n",
    "    'GeoRefNo': 'geographic_reference_number',\n",
    "    'Tier': 'tier_number',\n",
    "    'Range': 'range_number',\n",
    "    'Prop_Addr': 'property_address',\n",
    "    \n",
    "    # Zoning I\\nformation\n",
    "    'ZngCdPr': 'zoning_code_primary',\n",
    "    'ZngCdSc': 'zoning_code_secondary',\n",
    "    'ZngOLPr': 'zoning_overlay_primary',\n",
    "    'ZngOLSc': 'zoning_overlay_secondary',\n",
    "    'MSZoning': 'zoning_classification',\n",
    "    \n",
    "    # Classification and Legal\n",
    "    'ClassPr_S': 'property_class_primary',\n",
    "    'ClassSc_S': 'property_class_secondary',\n",
    "    'Legal_Pr': 'legal_description_primary',\n",
    "    'SchD_S': 'school_district',\n",
    "    'TxD_S': 'tax_district',\n",
    "    \n",
    "    # Owner Information\n",
    "    'MA_Ownr1': 'owner_name_1',\n",
    "    'MA_Ownr2': 'owner_name_2',\n",
    "    'MA_Line1': 'mailing_address_line1',\n",
    "    'MA_City': 'mailing_city',\n",
    "    'MA_State': 'mailing_state',\n",
    "    'MA_Zip1': 'mailing_zip',\n",
    "    'MA_Zip2': 'mailing_zip_plus4',\n",
    "    \n",
    "    # Record Information\n",
    "    'Rcrd_Yr': 'record_year',\n",
    "    'Rcrd_Mo': 'record_month',\n",
    "    'Inst1_No': 'instrument_number',\n",
    "    'Inst1_Yr': 'instrument_year',\n",
    "    'Inst1_Mo': 'instrument_month',\n",
    "    'Inst1TPr': 'instrument_type_primary',\n",
    "    \n",
    "    # Land Information\n",
    "    'LndAc_S': 'land_acres',\n",
    "    'ImpAc_S': 'improved_acres',\n",
    "    'OthAc_S': 'other_acres',\n",
    "    'TtlVal_AsrYr': 'total_value_assessor_year',\n",
    "    'ValType': 'valuation_type',\n",
    "    'X1TPr_D': 'tax_primary_dollars',\n",
    "    'X1TSc_D': 'tax_secondary_dollars',\n",
    "    'X1TPr_S': 'tax_primary_status',\n",
    "    'X1TSc_S': 'tax_secondary_status',\n",
    "    'LndAcX1S': 'land_acres_tax_1',\n",
    "    'ImpAcX1S': 'improved_acres_tax_1',\n",
    "    'ImpAcX2S': 'improved_acres_tax_2',\n",
    "    'HSTtl_D': 'homestead_total_dollars',\n",
    "    'MilVal_D': 'military_value_dollars',\n",
    "    'HSTtl_S': 'homestead_total_status',\n",
    "    'MilVal_S': 'military_value_status',\n",
    "    'AcreX_S1': 'acres_extra_status_1',\n",
    "    'AcreGr': 'acres_gross',\n",
    "    'AcreNt_S': 'acres_net_status',\n",
    "    'LotArea': 'lot_area_sqft',\n",
    "    'LotFrontage': 'lot_frontage_feet',\n",
    "    'ParType': 'parcel_type',\n",
    "    \n",
    "    # Building Information\n",
    "    'BldgNo_S': 'building_number',\n",
    "    'DwlgNo_S': 'dwelling_number',\n",
    "    'BldgType': 'building_type',\n",
    "    'YrBuilt': 'year_built',\n",
    "    'YearBuilt': 'year_built_verified',\n",
    "    'YearRemodAdd': 'year_remodeled',\n",
    "    'HouseStyle': 'house_style',\n",
    "    'MSSubClass': 'building_class',\n",
    "    \n",
    "    # Structure Features\n",
    "    'Foundation': 'foundation_type',\n",
    "    'RoofStyle': 'roof_style',\n",
    "    'RoofMatl': 'roof_material',\n",
    "    'Exterior1st': 'exterior_material_primary',\n",
    "    'Exterior2nd': 'exterior_material_secondary',\n",
    "    'MasVnrType': 'masonry_veneer_type',\n",
    "    'MasVnrArea': 'masonry_veneer_area',\n",
    "    \n",
    "    # Utilities and Systems\n",
    "    'Street': 'street_type',\n",
    "    'Alley': 'alley_access',\n",
    "    'Utilities': 'utility_type',\n",
    "    'Heating': 'heating_type',\n",
    "    'HeatingQC': 'heating_quality',\n",
    "    'CentralAir': 'has_central_air',\n",
    "    'Electrical': 'electrical_system',\n",
    "    \n",
    "    # Living Area Information\n",
    "    'GLA': 'gross_living_area',\n",
    "    'GrLivArea': 'gross_living_area_verified',\n",
    "    '1stFlrSF': 'first_floor_sqft',\n",
    "    '2ndFlrSF': 'second_floor_sqft',\n",
    "    'LowQualFinSF': 'low_quality_finished_sqft',\n",
    "    'TtlBsmtSF': 'total_basement_sqft',\n",
    "    'TotalBsmtSF': 'total_basement_sqft_verified',\n",
    "    \n",
    "    # Room Counts\n",
    "    'TotRmsAbvGrd': 'total_rooms_above_ground',\n",
    "    'BedroomAbvGr': 'bedrooms_above_ground',\n",
    "    'KitchenAbvGr': 'kitchens_above_ground',\n",
    "    \n",
    "    # Bathroom Counts\n",
    "    'BsmtFullBath': 'basement_full_bathrooms',\n",
    "    'BsmtHalfBath': 'basement_half_bathrooms',\n",
    "    'FullBath': 'full_bathrooms',\n",
    "    'HalfBath': 'half_bathrooms',\n",
    "    \n",
    "    # Quality Ratings\n",
    "    'OverallQual': 'overall_quality_rating',\n",
    "    'OverallCond': 'overall_condition_rating',\n",
    "    'ExterQual': 'exterior_quality',\n",
    "    'ExterCond': 'exterior_condition',\n",
    "    'BsmtQual': 'basement_quality',\n",
    "    'BsmtCond': 'basement_condition',\n",
    "    'KitchenQual': 'kitchen_quality',\n",
    "    \n",
    "    # Basement Details\n",
    "    'BsmtExposure': 'basement_exposure',\n",
    "    'BsmtFinType1': 'basement_finish_type_1',\n",
    "    'BsmtFinType2': 'basement_finish_type_2',\n",
    "    'BsmtFinSF1': 'basement_finished_sqft_1',\n",
    "    'BsmtFinSF2': 'basement_finished_sqft_2',\n",
    "    'BsmtUnfSF': 'basement_unfinished_sqft',\n",
    "    \n",
    "    # Garage Information\n",
    "    'GarageType': 'garage_type',\n",
    "    'GarageYrBlt': 'garage_year_built',\n",
    "    'GarYrBlt': 'garage_year_built_alt',\n",
    "    'GarageFinish': 'garage_interior_finish',\n",
    "    'GarageCars': 'garage_car_capacity',\n",
    "    'Cars': 'garage_car_capacity_alt',\n",
    "    'GarageArea': 'garage_area_sqft',\n",
    "    'GarageQual': 'garage_quality',\n",
    "    'GarageCond': 'garage_condition',\n",
    "    \n",
    "    # Additional Features\n",
    "    'Fireplaces': 'number_of_fireplaces',\n",
    "    'FireplaceQu': 'fireplace_quality',\n",
    "    'PoolArea': 'pool_area_sqft',\n",
    "    'Fence': 'fence_quality',\n",
    "    'MiscFeature': 'miscellaneous_feature',\n",
    "    'MiscVal': 'miscellaneous_value',\n",
    "    'WoodDeckSF': 'wood_deck_sqft',\n",
    "    'OpenPorchSF': 'open_porch_sqft',\n",
    "    'EnclosedPorch': 'enclosed_porch_sqft',\n",
    "    '3SsnPorch': 'three_season_porch_sqft',\n",
    "    'ScreenPorch': 'screen_porch_sqft',\n",
    "    'PavedDrive': 'paved_driveway',\n",
    "    \n",
    "    # Property and Lot Details\n",
    "    'LotShape': 'lot_shape',\n",
    "    'LandContour': 'land_contour',\n",
    "    'LotConfig': 'lot_configuration',\n",
    "    'LandSlope': 'land_slope',\n",
    "    'Neighborhood': 'neighborhood_name',\n",
    "    'Condition1': 'proximity_condition_1',\n",
    "    'Condition2': 'proximity_condition_2',\n",
    "    'Functional': 'home_functionality',\n",
    "    \n",
    "    # Sale Information\n",
    "    'YrSold_YYYY': 'year_sold',\n",
    "    'MoSold_MM': 'month_sold',\n",
    "    'YrSold': 'year_sold_verified',\n",
    "    'MoSold': 'month_sold_verified',\n",
    "    'SalePrice': 'sale_price',\n",
    "    'SaleType': 'sale_type',\n",
    "    'SaleCond': 'sale_condition',\n",
    "    'SaleCondition': 'sale_condition_verified',\n",
    "    \n",
    "    # Additional Identifiers\n",
    "    'ParclRel': 'parcel_relationship',\n",
    "    'PA-Nmbr': 'property_address_number',\n",
    "    'PA-PreD': 'property_address_pre_direction',\n",
    "    'PA-Strt': 'property_address_street',\n",
    "    'PA-StSfx': 'property_address_street_suffix',\n",
    "    'PA-UnTyp': 'property_address_unit_type',\n",
    "    'PA-UntNo': 'property_address_unit_number',\n",
    "    'Date': 'record_date',\n",
    "    'Source': 'data_source',\n",
    "    'NmbrBRs': 'number_bedrooms',\n",
    "    'PID': 'parcel_identification_number'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_outliers(file_path, column_mapping=None):\n",
    "    \"\"\"\n",
    "    Analyze outliers in numeric columns of a dataset with improved readability and efficiency.\n",
    "    Applies different IQR multipliers for columns with sparse data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the CSV file\n",
    "    column_mapping : dict, optional\n",
    "        Dictionary mapping original column names to new names\n",
    "    \"\"\"\n",
    "    # Read the data\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Apply column mapping if provided\n",
    "    if column_mapping:\n",
    "        df.rename(columns=column_mapping, inplace=True)\n",
    "    \n",
    "    # Select only numeric columns and cache the result\n",
    "    numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    \n",
    "    # Define columns that need special handling (sparse columns)\n",
    "    sparse_columns = ['basement_finished_sqft_2', 'enclosed_porch_sqft']\n",
    "    \n",
    "    def calculate_outlier_bounds(data, column_name):\n",
    "        \"\"\"Calculate outlier boundaries using IQR method with flexible multiplier.\"\"\"\n",
    "        clean_data = data.dropna()\n",
    "        if len(clean_data) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Use different IQR multiplier for sparse columns\n",
    "        iqr_multiplier = 1.0 if column_name in sparse_columns else 1.5\n",
    "        \n",
    "        Q1 = clean_data.quantile(0.25)\n",
    "        Q3 = clean_data.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        return {\n",
    "            'lower_bound': Q1 - iqr_multiplier * IQR,\n",
    "            'upper_bound': Q3 + iqr_multiplier * IQR,\n",
    "            'clean_data': clean_data,\n",
    "            'iqr_multiplier': iqr_multiplier\n",
    "        }\n",
    "    \n",
    "    def count_outliers(data, bounds):\n",
    "        \"\"\"Count outliers based on pre-calculated boundaries.\"\"\"\n",
    "        if bounds is None:\n",
    "            return 0\n",
    "        outliers = data[(data < bounds['lower_bound']) | (data > bounds['upper_bound'])]\n",
    "        return len(outliers)\n",
    "    \n",
    "    for column in numeric_columns:\n",
    "        clean_data = df[column].dropna()\n",
    "        \n",
    "        # Skip empty columns\n",
    "        if len(clean_data) == 0:\n",
    "            print(f\"\\nSkipping {column} - No valid data found\")\n",
    "            print(\"-\" * 50)\n",
    "            continue\n",
    "        \n",
    "        # Calculate bounds once\n",
    "        bounds = calculate_outlier_bounds(clean_data, column)\n",
    "        if bounds is None:\n",
    "            continue\n",
    "            \n",
    "        # Create plots\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Calculate outliers\n",
    "        outlier_count = count_outliers(clean_data, bounds)\n",
    "        \n",
    "        # Bar plot\n",
    "        ax1.bar(['Outliers'], [outlier_count], color='red', alpha=0.6)\n",
    "        ax1.set_title(f'Number of Outliers in {column}')\n",
    "        ax1.set_ylabel('Count')\n",
    "        \n",
    "        # Box plot with custom style\n",
    "        sns.boxplot(data=clean_data, ax=ax2, color='skyblue')\n",
    "        ax2.set_title(f'Distribution and Outliers in {column}')\n",
    "        \n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Add statistics annotation\n",
    "        percentage = (outlier_count/len(clean_data))*100\n",
    "        ax1.text(0, outlier_count/2, f'{percentage:.1f}%', \n",
    "                horizontalalignment='center', verticalalignment='center')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\nSummary for {column}:\")\n",
    "        print(f\"Total outliers: {outlier_count}\")\n",
    "        print(f\"Percentage of outliers: {percentage:.2f}%\")\n",
    "        print(f\"Total valid samples: {len(clean_data)}\")\n",
    "        print(f\"IQR multiplier used: {bounds['iqr_multiplier']}\")\n",
    "        if column in sparse_columns:\n",
    "            print(\"Note: Using reduced IQR multiplier (1.0) due to sparse data\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Call the function\n",
    "file_path = '/Users/jorgemartinez/Documents/NYDSA #3 Machine Learning Project/Ames Real Estate and House Price Data Merged.csv'\n",
    "analyze_outliers(file_path, column_mapping)  # Assuming column_mapping is defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PH: Lots of outliers for basement_finished_sqft_2 and enclosed_porch_sqft, because my guess is that few \n",
    "# of the rows in your dataset have a basement or an enclosed porch. I would define a new method to compute outliers for these two \n",
    "# columns, using 1*IQR instead of 1.5*IQR for instance, and explain in your writeup that you are capping the outlier numbers \n",
    "# given fewer samples in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define numeric columns with mapped names\n",
    "numeric_columns = [\n",
    "   'sale_price', 'gross_living_area_verified', 'lot_area_sqft',\n",
    "   'total_basement_sqft_verified', 'garage_area_sqft',\n",
    "   'total_rooms_above_ground', 'number_of_fireplaces'\n",
    "]\n",
    "\n",
    "def analyze_and_handle_outliers(df, numeric_columns):\n",
    "   # Methods definition\n",
    "   methods = {\n",
    "       'iqr': ['sale_price'],\n",
    "       'zscore': ['gross_living_area_verified', 'lot_area_sqft', \n",
    "                 'total_basement_sqft_verified', 'garage_area_sqft'],\n",
    "       'statistical': ['total_rooms_above_ground', 'number_of_fireplaces']\n",
    "   }\n",
    "   \n",
    "   # Calculate statistics\n",
    "   column_stats = {col: {\n",
    "       'quantiles': df[col].quantile([0.25, 0.75, 0.95]),\n",
    "       'mean': df[col].mean(),\n",
    "       'std': df[col].std()\n",
    "   } for col in numeric_columns if col in df.columns}\n",
    "   \n",
    "   def apply_capping(data, col, method):\n",
    "       stats = column_stats[col]\n",
    "       if method == 'iqr':\n",
    "           Q1, Q3 = stats['quantiles'][0.25], stats['quantiles'][0.75]\n",
    "           IQR = Q3 - Q1\n",
    "           return data.clip(lower=Q1 - 1.5 * IQR, upper=Q3 + 1.5 * IQR)\n",
    "       elif method == 'zscore':\n",
    "           return data.clip(lower=stats['mean'] - 3 * stats['std'], \n",
    "                          upper=stats['mean'] + 3 * stats['std'])\n",
    "       return data.clip(upper=stats['quantiles'][0.95])\n",
    "   \n",
    "   for method, columns in methods.items():\n",
    "       for col in columns:\n",
    "           if col in df.columns:\n",
    "               df[f'{col}_processed'] = apply_capping(df[col], col, method)\n",
    "   \n",
    "   # Plotting comparisons\n",
    "   columns_to_plot = [\n",
    "       ('sale_price', 'sale_price_processed', 'Sale Price'),\n",
    "       ('gross_living_area_verified', 'gross_living_area_verified_processed', 'Living Area'),\n",
    "       ('lot_area_sqft', 'lot_area_sqft_processed', 'Lot Area'),\n",
    "       ('total_basement_sqft_verified', 'total_basement_sqft_verified_processed', 'Basement SF'),\n",
    "       ('garage_area_sqft', 'garage_area_sqft_processed', 'Garage Area')\n",
    "   ]\n",
    "   \n",
    "   for orig_col, proc_col, title in columns_to_plot:\n",
    "       if orig_col in df.columns and proc_col in df.columns:\n",
    "           plot_comparison(df, orig_col, proc_col, title)\n",
    "   \n",
    "   return df\n",
    "\n",
    "def plot_comparison(data, original_col, processed_col, title):\n",
    "   fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "   \n",
    "   overall_min = min(data[original_col].min(), data[processed_col].min())\n",
    "   overall_max = max(data[original_col].max(), data[processed_col].max())\n",
    "   \n",
    "   sns.boxplot(x=data[original_col], ax=ax1)\n",
    "   ax1.set_title(f'Original {title}')\n",
    "   ax1.set_xlim(overall_min, overall_max)\n",
    "   \n",
    "   sns.boxplot(x=data[processed_col], ax=ax2)\n",
    "   ax2.set_title(f'Processed {title}')\n",
    "   ax2.set_xlim(overall_min, overall_max)\n",
    "   \n",
    "   plt.tight_layout()\n",
    "   plt.show()\n",
    "   \n",
    "   print(f\"\\nStats for {title}:\")\n",
    "   print(f\"Original range: {data[original_col].min():.2f} to {data[original_col].max():.2f}\")\n",
    "   print(f\"Processed range: {data[processed_col].min():.2f} to {data[processed_col].max():.2f}\")\n",
    "   print(f\"Original mean: {data[original_col].mean():.2f}\")\n",
    "   print(f\"Processed mean: {data[processed_col].mean():.2f}\")\n",
    "   print(f\"Original std: {data[original_col].std():.2f}\")\n",
    "   print(f\"Processed std: {data[processed_col].std():.2f}\")\n",
    "   print(f\"Values modified: {(data[original_col] != data[processed_col]).sum()}\")\n",
    "\n",
    "# Execute the analysis\n",
    "df = pd.read_csv('/Users/jorgemartinez/Documents/NYDSA #3 Machine Learning Project/Ames Real Estate and House Price Data Merged.csv', \n",
    "                low_memory=False)\n",
    "df.rename(columns=column_mapping, inplace=True)\n",
    "processed_df = analyze_and_handle_outliers(df, numeric_columns)\n",
    "processed_df.to_csv('/Users/jorgemartinez/Documents/NYDSA #3 Machine Learning Project/Ames_Housing_Processed.csv', \n",
    "                  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_missing_values(df, column_mapping, top_n=20):\n",
    "    \"\"\"\n",
    "    Comprehensive missing values analysis with optimized performance\n",
    "    \"\"\"\n",
    "    # Apply column mapping\n",
    "    df = df.rename(columns=column_mapping)\n",
    "    \n",
    "    # Calculate missing values once\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_percentage = (missing_values / len(df)) * 100\n",
    "    \n",
    "    # Create missing values DataFrame\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Column': missing_values.index,\n",
    "        'Missing Count': missing_values.values,\n",
    "        'Missing Percentage': missing_percentage.values\n",
    "    }).sort_values('Missing Count', ascending=False)\n",
    "    missing_df = missing_df[missing_df['Missing Count'] > 0]\n",
    "    \n",
    "    # Create visualization subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))\n",
    "    \n",
    "    # Box plot\n",
    "    ax1.boxplot(missing_values)\n",
    "    ax1.set_ylabel('Number of Missing Values')\n",
    "    ax1.set_title('Distribution of Missing Values Across Columns')\n",
    "    \n",
    "    # Bar plot of top N columns\n",
    "    top_missing = missing_df.head(top_n)\n",
    "    bars = ax2.barh(range(len(top_missing)), top_missing['Missing Percentage'], color='steelblue')\n",
    "    ax2.set_yticks(range(len(top_missing)))\n",
    "    ax2.set_yticklabels(top_missing['Column'])\n",
    "    ax2.set_xlabel('Percentage of Missing Values')\n",
    "    ax2.set_title(f'Top {top_n} Columns with Missing Values')\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        ax2.text(width/2, bar.get_y() + bar.get_height()/2,\n",
    "                f'{width:.1f}%',\n",
    "                ha='center', va='center',\n",
    "                color='white' if width > 30 else 'black',\n",
    "                fontweight='bold')\n",
    "    \n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Generate summary statistics\n",
    "    ranges = [0, 1, 5, 10, 25, 50, 75, 100]\n",
    "    labels = ['<1%', '1-5%', '5-10%', '10-25%', '25-50%', '50-75%', '>75%']\n",
    "    missing_df['Range'] = pd.cut(missing_df['Missing Percentage'], bins=ranges, labels=labels, right=False)\n",
    "    \n",
    "    stats = {\n",
    "        'total_columns_missing': len(missing_df),\n",
    "        'max_missing': missing_df['Missing Count'].max(),\n",
    "        'avg_missing': missing_df['Missing Count'].mean(),\n",
    "        'top_10_missing': missing_df.head(10)[['Column', 'Missing Count', 'Missing Percentage']],\n",
    "        'distribution': missing_df['Range'].value_counts().sort_index()\n",
    "    }\n",
    "    \n",
    "    return missing_df, stats, fig\n",
    "\n",
    "# Read and analyze data\n",
    "file_path = '/Users/jorgemartinez/Documents/NYDSA #3 Machine Learning Project/Ames Real Estate and House Price Data Merged.csv'\n",
    "df = pd.read_csv(file_path, low_memory=False)\n",
    "missing_df, stats, fig = analyze_missing_values(df, column_mapping)\n",
    "\n",
    "# Display results\n",
    "plt.show()\n",
    "print(\"\\nMissing Values Summary:\")\n",
    "print(f\"Total columns with missing values: {stats['total_columns_missing']}\")\n",
    "print(f\"Maximum missing values: {stats['max_missing']}\")\n",
    "print(f\"Average missing values: {stats['avg_missing']:.2f}\")\n",
    "print(\"\\nTop 10 Columns with Most Missing Values:\")\n",
    "print(stats['top_10_missing'])\n",
    "print(\"\\nDistribution of Missing Values:\")\n",
    "print(stats['distribution'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Set, Optional, Tuple\n",
    "\n",
    "class FeatureEngineer:\n",
    "    def __init__(self, file_path: str, column_mapping: Dict):\n",
    "        self.file_path = file_path\n",
    "        self.column_mapping = column_mapping\n",
    "        self.quality_map = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1}\n",
    "        self.exposure_map = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'None': 0}\n",
    "        \n",
    "    def load_and_process(self) -> Optional[pd.DataFrame]:\n",
    "        try:\n",
    "            df = pd.read_csv(self.file_path, low_memory=False)\n",
    "            df = df.rename(columns=self.column_mapping)\n",
    "            return self._engineer_features(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _engineer_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        original_features = set(df.columns)\n",
    "        df = df.copy()\n",
    "        \n",
    "        transformations = {\n",
    "            'time_features': self._create_time_features,\n",
    "            'quality_features': self._create_quality_features,\n",
    "            'space_features': self._create_space_features,\n",
    "            'location_features': self._create_location_features,\n",
    "            'functional_features': self._create_functional_features\n",
    "        }\n",
    "        \n",
    "        for transform in transformations.values():\n",
    "            df = transform(df)\n",
    "            \n",
    "        df = self._cleanup_and_validate(df)\n",
    "        self._print_feature_summary(original_features, set(df.columns))\n",
    "        return df\n",
    "\n",
    "    def _create_time_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df['house_age'] = df['year_sold_verified'] - df['year_built_verified']\n",
    "        df['years_since_remodel'] = df['year_sold_verified'] - df['year_remodeled']\n",
    "        df['renovation_indicator'] = (df['year_built_verified'] != df['year_remodeled']).astype(int)\n",
    "        \n",
    "        # Handle missing values before conversion\n",
    "        df['sale_season'] = (df['month_sold_verified']\n",
    "                            .fillna(1)  # Fill NA values with 1 (first quarter)\n",
    "                            .astype(float)\n",
    "                            .sub(1)     # Subtract 1 \n",
    "                            .floordiv(3)  # Integer division by 3\n",
    "                            .add(1)     # Add 1 to get quarters 1-4\n",
    "                            .astype(int))\n",
    "        return df\n",
    "\n",
    "    def _create_quality_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        quality_columns = {\n",
    "            'exterior': ['exterior_quality', 'exterior_condition'],\n",
    "            'basement': ['basement_quality', 'basement_condition'],\n",
    "            'garage': ['garage_quality', 'garage_condition']\n",
    "        }\n",
    "        \n",
    "        for feature, cols in quality_columns.items():\n",
    "            for col in cols:\n",
    "                df[f\"{col}_num\"] = df[col].map(self.quality_map)\n",
    "            df[f\"{feature}_score\"] = df[[f\"{col}_num\" for col in cols]].mean(axis=1)\n",
    "        \n",
    "        df['quality_condition_score'] = df['overall_quality_rating'] * df['overall_condition_rating']\n",
    "        df['kitchen_score'] = df['kitchen_quality'].map(self.quality_map) * df['kitchens_above_ground']\n",
    "        return df\n",
    "\n",
    "    def _create_space_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df['total_sqft'] = (df['gross_living_area_verified'] + \n",
    "                           df['garage_area_sqft'].fillna(0) + \n",
    "                           df['total_basement_sqft_verified'].fillna(0))\n",
    "        \n",
    "        df['living_space_efficiency'] = df['gross_living_area_verified'] / df['lot_area_sqft'].where(df['lot_area_sqft'] > 0, 1)\n",
    "        \n",
    "        outdoor_cols = ['wood_deck_sqft', 'open_porch_sqft', 'enclosed_porch_sqft', \n",
    "                       'three_season_porch_sqft', 'screen_porch_sqft']\n",
    "        df['total_outdoor_sqft'] = df[outdoor_cols].fillna(0).sum(axis=1)\n",
    "        return df\n",
    "\n",
    "    def _create_location_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df['neighborhood_mean_price'] = df.groupby('neighborhood_name')['sale_price'].transform('mean')\n",
    "        df['lot_frontage_ratio'] = df['lot_frontage_feet'] / df['lot_area_sqft'].where(df['lot_area_sqft'] > 0, 1)\n",
    "        return df\n",
    "\n",
    "    def _create_functional_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df['total_baths'] = (df['full_bathrooms'] + 0.5 * df['half_bathrooms'] + \n",
    "                            df['basement_full_bathrooms'] + 0.5 * df['basement_half_bathrooms'])\n",
    "        df['bedroom_bath_ratio'] = df['bedrooms_above_ground'] / df['total_baths'].where(df['total_baths'] > 0, 1)\n",
    "        df['has_fence'] = (~df['fence_quality'].isna()).astype(int)\n",
    "        return df\n",
    "\n",
    "    def _cleanup_and_validate(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.drop(columns=[col for col in df.columns if col.endswith('_num')])\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numeric_cols] = df[numeric_cols].replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            if col != 'sale_price' and df[col].isnull().any():\n",
    "                df[col] = df[col].fillna(0 if 'ratio' in col or 'score' in col else df[col].median())\n",
    "        return df\n",
    "\n",
    "    def _print_feature_summary(self, original_features: Set[str], final_features: Set[str]) -> None:\n",
    "        new_features = final_features - original_features\n",
    "        print(f\"\\nFeature Engineering Summary:\")\n",
    "        print(f\"Original features: {len(original_features)}\")\n",
    "        print(f\"New features: {len(new_features)}\")\n",
    "        print(\"\\nEngineered features:\")\n",
    "        for feature in sorted(new_features):\n",
    "            print(f\"- {feature}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    engineer = FeatureEngineer('/Users/jorgemartinez/Documents/NYDSA #3 Machine Learning Project/Ames Real Estate and House Price Data Merged.csv', column_mapping)\n",
    "    df_engineered = engineer.load_and_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Set, Optional\n",
    "\n",
    "class FeatureEngineer:\n",
    "    def __init__(self, file_path: str, column_mapping: Dict):\n",
    "        self.file_path = file_path\n",
    "        self.column_mapping = column_mapping\n",
    "        self.quality_map = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1} #TODO: This is fine to do to feature engineer other variables, like I see you multiplying this variable by another to create a \"kitchen_score\" later on. However, do not use this variable in your dataset used to train your machine learning model - the model won't be able to use these variables when training. Instead, you should binary encode each of the values in this vriable if you do want to use this column in your model.\n",
    "        self.exposure_map = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'None': 0} #TODO: Same thing here\n",
    "        \n",
    "    def load_and_process(self) -> Optional[pd.DataFrame]:\n",
    "        try:\n",
    "            df = pd.read_csv(self.file_path, low_memory=False)\n",
    "            df = df.rename(columns=self.column_mapping)\n",
    "            return self._engineer_features(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _engineer_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        original_features = set(df.columns)\n",
    "        df = df.copy()\n",
    "        \n",
    "        transformations = {\n",
    "            'time_features': self._create_time_features,\n",
    "            'quality_features': self._create_quality_features,\n",
    "            'space_features': self._create_space_features,\n",
    "            'location_features': self._create_location_features,\n",
    "            'functional_features': self._create_functional_features\n",
    "        }\n",
    "        \n",
    "        for transform in transformations.values():\n",
    "            df = transform(df)\n",
    "            \n",
    "        df = self._cleanup_and_validate(df)\n",
    "        self._print_feature_summary(original_features, set(df.columns))\n",
    "        return df\n",
    "\n",
    "    def _create_time_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        try:\n",
    "            df['house_age'] = df['year_sold_verified'] - df['year_built_verified']\n",
    "            df['years_since_remodel'] = df['year_sold_verified'] - df['year_remodeled']\n",
    "            df['renovation_indicator'] = (df['year_built_verified'] != df['year_remodeled']).astype(int)\n",
    "            \n",
    "            # Handle NA values before conversion\n",
    "            df['month_sold_verified'] = pd.to_numeric(df['month_sold_verified'], errors='coerce')\n",
    "            df['month_sold_verified'] = df['month_sold_verified'].fillna(1)  # Default to January for missing values\n",
    "            df['sale_season'] = pd.to_datetime(df['month_sold_verified'].astype(int).astype(str), format='%m').dt.quarter\n",
    "        except Exception as e:\n",
    "            print(f\"Error in time features: {e}\")\n",
    "        return df\n",
    "\n",
    "    def _create_quality_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        quality_columns = {\n",
    "            'exterior': ['exterior_quality', 'exterior_condition'],\n",
    "            'basement': ['basement_quality', 'basement_condition'],\n",
    "            'garage': ['garage_quality', 'garage_condition']\n",
    "        }\n",
    "        \n",
    "        for feature, cols in quality_columns.items():\n",
    "            if all(col in df.columns for col in cols):\n",
    "                for col in cols:\n",
    "                    df[f\"{col}_num\"] = df[col].map(self.quality_map)\n",
    "                df[f\"{feature}_score\"] = df[[f\"{col}_num\" for col in cols]].mean(axis=1)\n",
    "        \n",
    "        df['quality_condition_score'] = df['overall_quality_rating'] * df['overall_condition_rating']\n",
    "        df['kitchen_score'] = df['kitchen_quality'].map(self.quality_map) * df['kitchens_above_ground']\n",
    "        return df\n",
    "\n",
    "    def _create_space_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df['total_sqft'] = (df['gross_living_area_verified'] + \n",
    "                           df['garage_area_sqft'].fillna(0) + \n",
    "                           df['total_basement_sqft_verified'].fillna(0))\n",
    "        \n",
    "        df['living_space_efficiency'] = df['gross_living_area_verified'] / df['lot_area_sqft'].where(df['lot_area_sqft'] > 0, 1)\n",
    "        \n",
    "        outdoor_cols = ['wood_deck_sqft', 'open_porch_sqft', 'enclosed_porch_sqft', \n",
    "                       'three_season_porch_sqft', 'screen_porch_sqft']\n",
    "        df['total_outdoor_sqft'] = df[outdoor_cols].fillna(0).sum(axis=1)\n",
    "        return df\n",
    "\n",
    "    def _create_location_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df['neighborhood_mean_price'] = df.groupby('neighborhood_name')['sale_price'].transform('mean')\n",
    "        df['lot_frontage_ratio'] = df['lot_frontage_feet'] / df['lot_area_sqft'].where(df['lot_area_sqft'] > 0, 1)\n",
    "        return df\n",
    "\n",
    "    def _create_functional_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df['total_baths'] = (df['full_bathrooms'] + 0.5 * df['half_bathrooms'] + \n",
    "                            df['basement_full_bathrooms'] + 0.5 * df['basement_half_bathrooms'])\n",
    "        \n",
    "        df['bedroom_bath_ratio'] = df['bedrooms_above_ground'] / df['total_baths'].where(df['total_baths'] > 0, 1)\n",
    "        df['has_fence'] = (~df['fence_quality'].isna()).astype(int)\n",
    "        return df\n",
    "\n",
    "    def _cleanup_and_validate(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Keep your existing cleanup code\n",
    "        df = df.drop(columns=[col for col in df.columns if col.endswith('_num')])\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numeric_cols] = df[numeric_cols].replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        # Add this new section for binary encoding\n",
    "        quality_columns = ['exterior_quality', 'exterior_condition', \n",
    "                        'basement_quality', 'basement_condition',\n",
    "                        'garage_quality', 'garage_condition',\n",
    "                        'kitchen_quality']\n",
    "        \n",
    "        # Only encode columns that exist in the dataframe\n",
    "        cols_to_encode = [col for col in quality_columns if col in df.columns]\n",
    "        \n",
    "        if cols_to_encode:\n",
    "            encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "            \n",
    "            for col in cols_to_encode:\n",
    "                # Fit and transform the encoder\n",
    "                encoded_features = encoder.fit_transform(df[[col]])\n",
    "                \n",
    "                # Create feature names\n",
    "                feature_names = [f\"{col}_{cat}\" for cat in encoder.categories_[0]]\n",
    "                \n",
    "                # Add the encoded features to the dataframe\n",
    "                encoded_df = pd.DataFrame(encoded_features, columns=feature_names, index=df.index)\n",
    "                df = pd.concat([df, encoded_df], axis=1)\n",
    "                \n",
    "                # Optionally remove the original column\n",
    "                df = df.drop(columns=[col])\n",
    "        \n",
    "        # Keep your existing null handling\n",
    "        for col in numeric_cols:\n",
    "            if col != 'sale_price' and df[col].isnull().any():\n",
    "                df[col] = df[col].fillna(0 if 'ratio' in col or 'score' in col else df[col].median())\n",
    "                \n",
    "        return df\n",
    "\n",
    "    def _print_feature_summary(self, original_features: Set[str], final_features: Set[str]) -> None:\n",
    "        new_features = final_features - original_features\n",
    "        print(f\"\\nFeature Engineering Summary:\")\n",
    "        print(f\"Original features: {len(original_features)}\")\n",
    "        print(f\"New features: {len(new_features)}\")\n",
    "        print(\"\\nEngineered features:\")\n",
    "        for feature in sorted(new_features):\n",
    "            print(f\"- {feature}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    engineer = FeatureEngineer('/Users/jorgemartinez/Documents/NYDSA #3 Machine Learning Project/Ames Real Estate and House Price Data Merged.csv', column_mapping)\n",
    "    df_engineered = engineer.load_and_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "def analyze_feature_scales(df, target_col='SalePrice'):\n",
    "    \"\"\"\n",
    "    Analyzes features to determine if standardization is necessary.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input dataset\n",
    "    target_col (str): Name of the target variable\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (DataFrame with statistics, dict with recommendations)\n",
    "    \"\"\"\n",
    "    # Remove non-numeric columns\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Calculate statistics for each numeric column\n",
    "    stats_df = pd.DataFrame({\n",
    "        'mean': numeric_df.mean(),\n",
    "        'std': numeric_df.std(),\n",
    "        'min': numeric_df.min(),\n",
    "        'max': numeric_df.max(),\n",
    "        'range': numeric_df.max() - numeric_df.min(),\n",
    "        'skewness': numeric_df.skew()\n",
    "    })\n",
    "    \n",
    "    # Calculate scale ratios between features\n",
    "    max_range = stats_df['range'].max()\n",
    "    min_range = stats_df['range'].min()\n",
    "    scale_ratio = max_range / min_range if min_range != 0 else float('inf')\n",
    "    \n",
    "    # Identify highly skewed features\n",
    "    skewed_features = stats_df[abs(stats_df['skewness']) > 1].index.tolist()\n",
    "    \n",
    "    # Calculate correlations with target\n",
    "    correlations = numeric_df.corr()[target_col].sort_values(ascending=False)\n",
    "    \n",
    "    # Generate recommendations\n",
    "    recommendations = {\n",
    "        'standardization_needed': scale_ratio > 10,\n",
    "        'reason': f\"Scale ratio between largest and smallest feature ranges is {scale_ratio:.2f}\",\n",
    "        'skewed_features': skewed_features,\n",
    "        'top_correlations': correlations.head(5).to_dict(),\n",
    "        'suggested_transforms': {}\n",
    "    }\n",
    "    \n",
    "    # Suggest transformations for skewed features\n",
    "    for feature in skewed_features:\n",
    "        skew = stats_df.loc[feature, 'skewness']\n",
    "        if skew > 1:\n",
    "            recommendations['suggested_transforms'][feature] = 'log or sqrt transform'\n",
    "        elif skew < -1:\n",
    "            recommendations['suggested_transforms'][feature] = 'square or cube transform'\n",
    "    \n",
    "    return stats_df, recommendations\n",
    "\n",
    "\n",
    "#TODO: Other tests that I would also recommend to check normality and variance, and how to address these with Box-Cox transformations:\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "# TODO: You can combine these using the below function:\n",
    "def analyze_features(df, target_col='SalePrice'):\n",
    "    analysis = {\n",
    "        'basic_stats': df.describe(),\n",
    "        'normality': check_normality(df),\n",
    "        'zero_variance': check_variance(df),\n",
    "        'correlations': df.corr()[target_col].sort_values(ascending=False),\n",
    "        'missing_values': df.isnull().sum(),\n",
    "        'cardinality': df.nunique()\n",
    "    }\n",
    "    \n",
    "    # Check for multicollinearity\n",
    "    correlation_matrix = df.corr()\n",
    "    high_correlation = np.where(np.abs(correlation_matrix) > 0.8)\n",
    "    high_correlation = [(correlation_matrix.index[x], correlation_matrix.columns[y], correlation_matrix.iloc[x, y]) \n",
    "                       for x, y in zip(*high_correlation) if x != y]\n",
    "    \n",
    "    analysis['multicollinearity'] = high_correlation\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "\n",
    "def plot_distributions(df, n_features=5):\n",
    "    \"\"\"\n",
    "    Plots distributions of top correlated numeric features.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input dataset\n",
    "    n_features (int): Number of features to plot\n",
    "    \"\"\"\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    correlations = abs(numeric_df.corr()['SalePrice']).sort_values(ascending=False)\n",
    "    top_features = correlations[1:n_features+1].index  # Exclude target variable\n",
    "    \n",
    "    fig, axes = plt.subplots(n_features, 1, figsize=(10, 4*n_features))\n",
    "    for ax, feature in zip(axes, top_features):\n",
    "        sns.histplot(data=df, x=feature, ax=ax)\n",
    "        ax.set_title(f'{feature} Distribution')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your dataset\n",
    "    df = pd.read_csv('/Users/jorgemartinez/Documents/NYDSA #3 Machine Learning Project/Ames Real Estate and House Price Data Merged.csv')\n",
    "    \n",
    "    # Run analysis\n",
    "    stats, recommendations = analyze_feature_scales(df)\n",
    "    \n",
    "    # Print recommendations\n",
    "    print(\"\\nAnalysis Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Standardization needed: {recommendations['standardization_needed']}\")\n",
    "    print(f\"Reason: {recommendations['reason']}\")\n",
    "    print(\"\\nSkewed features requiring transformation:\")\n",
    "    for feature, transform in recommendations['suggested_transforms'].items():\n",
    "        print(f\"- {feature}: {transform}\")\n",
    "    \n",
    "    print(\"\\nTop correlations with Sale Price:\")\n",
    "    for feature, corr in recommendations['top_correlations'].items():\n",
    "        print(f\"- {feature}: {corr:.3f}\")\n",
    "    \n",
    "    # Create distribution plots\n",
    "    plot_distributions(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "House Price Feature Standardization with improved feature handling, duplicate removal, and skewness transformation\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def detect_numeric_categorical(df, column, threshold=15):\n",
    "    \"\"\"\n",
    "    Detects if a numeric column is likely categorical based on various criteria.\n",
    "    \"\"\"\n",
    "    unique_count = df[column].nunique()\n",
    "    \n",
    "    # Patterns suggesting categorical data\n",
    "    categorical_patterns = [\n",
    "        '_MM', '_YYYY', 'Nbr', 'Number', 'Count', 'Bath', 'Room', 'Year',\n",
    "        'Yr', 'Quality', 'Qual', 'Condition', 'Cond', 'Type', 'Grade',\n",
    "        'Mo_', '_Mo', 'Month', 'Zip', 'Code', 'Pool', 'Misc', 'ID', 'RefNo',\n",
    "        'Car', 'Tier', 'Range'\n",
    "    ]\n",
    "    \n",
    "    # Patterns suggesting continuous measurements\n",
    "    continuous_patterns = [\n",
    "        'SF', 'Area', 'SqFt', 'Footage', 'Acre', 'Val_', '_Val'\n",
    "    ]\n",
    "    \n",
    "    # Check if values are all integers when cast (allowing for NaN)\n",
    "    values = df[column].dropna()\n",
    "    all_ints = all(float(x).is_integer() for x in values)\n",
    "    \n",
    "    # Single value or very few values should be categorical\n",
    "    if unique_count <= 2:\n",
    "        return True\n",
    "    \n",
    "    # Check patterns\n",
    "    categorical_match = any(pattern.lower() in column.lower() for pattern in categorical_patterns)\n",
    "    continuous_match = any(pattern.lower() in column.lower() for pattern in continuous_patterns)\n",
    "    \n",
    "    # If it matches a continuous pattern, it's not categorical\n",
    "    if continuous_match:\n",
    "        return False\n",
    "    \n",
    "    # If it matches a categorical pattern or has few unique integer values, it's categorical\n",
    "    return categorical_match or (unique_count <= threshold and all_ints)\n",
    "\n",
    "def standardize_continuous_features(df, categorical_threshold=15):\n",
    "    \"\"\"\n",
    "    Standardizes continuous numeric features while preserving categorical ones.\n",
    "    Includes transformation of skewed features before standardization.\n",
    "    \"\"\"\n",
    "    # Known categorical numeric features\n",
    "    known_categorical = {\n",
    "        'MSSubClass', 'YrBuilt', 'YearBuilt', 'YearRemodAdd', \n",
    "        'GarYrBlt',  # Only one version of garage year\n",
    "        'MoSold', 'YrSold', 'Rcrd_Yr', 'Inst1_Yr',\n",
    "        'OverallQual', 'OverallCond', 'TotRmsAbvGrd', 'Fireplaces',\n",
    "        'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n",
    "        'BedroomAbvGr', 'KitchenAbvGr', 'GarageCars', 'Cars', 'NmbrBRs',\n",
    "        'MoSold_MM', 'YrSold_YYYY', 'Rcrd_Mo', 'Inst1_Mo',\n",
    "        'MA_Zip1', 'MA_Zip2', 'PoolArea', 'MiscVal',\n",
    "        'PID', 'GeoRefNo', 'MapRefNo',  # IDs\n",
    "        'MilVal_D', 'MilVal_S',  # Single value fields\n",
    "        'Range', 'Tier'  # Single value fields\n",
    "    }\n",
    "    \n",
    "    # Known continuous features (override automatic detection)\n",
    "    known_continuous = {\n",
    "        'TtlVal_AsrYr',\n",
    "        'HSTtl_D',     # Tax/value related\n",
    "        'HSTtl_S',     # Tax/value related\n",
    "        'LowQualFinSF',  # Square footage\n",
    "        '3SsnPorch',     # Porch measurements\n",
    "        'EnclosedPorch',\n",
    "        'ScreenPorch'\n",
    "    }\n",
    "    \n",
    "    # Remove duplicate columns\n",
    "    columns_to_drop = [col for col in df.columns if col.endswith('.1')]\n",
    "    if 'TotalBsmtSF' in df.columns and 'TtlBsmtSF' in df.columns:\n",
    "        columns_to_drop.append('TotalBsmtSF')\n",
    "    \n",
    "    df = df.drop(columns=columns_to_drop)\n",
    "    \n",
    "    # Get numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    # Identify categorical numeric features\n",
    "    categorical_numeric = set()\n",
    "    for col in numeric_cols:\n",
    "        if (col in known_categorical or \n",
    "            (detect_numeric_categorical(df, col, categorical_threshold) and \n",
    "             col not in known_continuous and\n",
    "             'Unnamed' not in col)):\n",
    "            categorical_numeric.add(col)\n",
    "    \n",
    "    # Identify continuous features\n",
    "    continuous_features = [col for col in numeric_cols \n",
    "                         if col not in categorical_numeric \n",
    "                         and col != 'SalePrice'\n",
    "                         and 'Unnamed' not in col]\n",
    "    \n",
    "    # Create copy of dataframe\n",
    "    df_standardized = df.copy()\n",
    "    \n",
    "    # Print analysis before standardization\n",
    "    print(\"\\nFeature Analysis:\")\n",
    "    print(\"\\nDuplicate Columns Removed:\")\n",
    "    for col in columns_to_drop:\n",
    "        print(f\"- {col}\")\n",
    "    \n",
    "    print(\"\\nColumns Requiring Investigation:\")\n",
    "    for col in [c for c in df.columns if 'Unnamed' in c]:\n",
    "        print(f\"- {col}: {df[col].nunique()} unique values\")\n",
    "        \n",
    "    print(\"\\nCategorical Numeric Features (not standardized):\")\n",
    "    for col in sorted(categorical_numeric):\n",
    "        unique_vals = sorted(df[col].unique())\n",
    "        print(f\"- {col}: {len(unique_vals)} unique values {unique_vals[:5]}{'...' if len(unique_vals) > 5 else ''}\")\n",
    "    \n",
    "    print(\"\\nContinuous Features (standardized):\")\n",
    "    for col in sorted(continuous_features):\n",
    "        print(f\"- {col}: {df[col].nunique()} unique values\")\n",
    "\n",
    "    # NEW CODE: Transform skewed features before standardization\n",
    "    skewed_features = [\n",
    "        'SchD_S', 'TxD_S', 'LndAc_S', 'ImpAc_S', 'OthAc_S', \n",
    "        'TtlVal_AsrYr', 'HSTtl_D', 'HSTtl_S', 'LotArea'\n",
    "    ]\n",
    "    \n",
    "    year_features = ['Rcrd_Yr', 'Inst1_Yr']\n",
    "    \n",
    "    print(\"\\nTransforming Skewed Features:\")\n",
    "    # Transform skewed continuous features that exist in our dataset\n",
    "    for feature in skewed_features:\n",
    "        if feature in continuous_features and feature in df.columns:\n",
    "            print(f\"- Applying log transform to {feature}\")\n",
    "            df_standardized[feature] = np.log1p(df_standardized[feature])\n",
    "    \n",
    "    for feature in year_features:\n",
    "        if feature in continuous_features and feature in df.columns:\n",
    "            print(f\"- Applying square transform to {feature}\")\n",
    "            df_standardized[feature] = df_standardized[feature] ** 2\n",
    "    \n",
    "    # Standardize the transformed continuous features\n",
    "    if continuous_features:\n",
    "        scaler = StandardScaler()\n",
    "        df_standardized[continuous_features] = scaler.fit_transform(df_standardized[continuous_features])\n",
    "    \n",
    "    return df_standardized\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv('/Users/jorgemartinez/Documents/NYDSA #3 Machine Learning Project/Ames Real Estate and House Price Data Merged.csv', \n",
    "                     low_memory=False)\n",
    "    df_standardized = standardize_continuous_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "class HousingEnsemble:\n",
    "    \"\"\"\n",
    "    Ensemble method for combining multiple models' predictions\n",
    "    with optimal weights determined through validation performance\n",
    "    \"\"\"\n",
    "    def __init__(self, models_dict, weight_method='performance'):\n",
    "        self.models = models_dict\n",
    "        self.weights = None\n",
    "        self.weight_method = weight_method\n",
    "        \n",
    "    def _calculate_weights(self, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Calculate weights for each model based on their validation performance\n",
    "        \"\"\"\n",
    "        performances = {}\n",
    "        \n",
    "        # Get predictions from each model\n",
    "        for name, (model, use_scaled) in self.models.items():\n",
    "            val_pred = model.predict(X_val)\n",
    "            r2 = r2_score(y_val, val_pred)\n",
    "            performances[name] = max(r2, 0)  # Ensure non-negative weights\n",
    "            \n",
    "        # Normalize weights to sum to 1\n",
    "        total = sum(performances.values())\n",
    "        self.weights = {name: score/total for name, score in performances.items()}\n",
    "        \n",
    "        return self.weights\n",
    "    \n",
    "    def fit(self, X_train, X_train_scaled, y_train):\n",
    "        \"\"\"\n",
    "        Fit the ensemble by calculating optimal weights\n",
    "        \"\"\"\n",
    "        # Use a portion of training data as validation set\n",
    "        val_size = int(0.2 * len(X_train))\n",
    "        X_val = X_train_scaled[-val_size:]\n",
    "        y_val = y_train[-val_size:]\n",
    "        \n",
    "        # Calculate weights based on validation performance\n",
    "        self.weights = self._calculate_weights(X_val, y_val)\n",
    "        \n",
    "        print(\"Ensemble weights:\")\n",
    "        for name, weight in self.weights.items():\n",
    "            print(f\"{name}: {weight:.4f}\")\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, X_scaled):\n",
    "        \"\"\"\n",
    "        Make predictions using weighted average of model predictions\n",
    "        \"\"\"\n",
    "        if self.weights is None:\n",
    "            raise ValueError(\"Ensemble must be fitted before making predictions\")\n",
    "            \n",
    "        weighted_predictions = []\n",
    "        \n",
    "        for name, (model, use_scaled) in self.models.items():\n",
    "            # Use appropriate scaled/unscaled data for each model\n",
    "            X_pred = X_scaled if use_scaled else X\n",
    "            predictions = model.predict(X_pred)\n",
    "            weighted_predictions.append(predictions * self.weights[name])\n",
    "            \n",
    "        # Combine predictions\n",
    "        final_predictions = np.sum(weighted_predictions, axis=0)\n",
    "        return final_predictions\n",
    "\n",
    "class AmesHousingML:\n",
    "    \"\"\"\n",
    "    Enhanced ML pipeline for Ames Housing that works with pre-processed data\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        self.label_encoders = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        self._prepare_data()\n",
    "        \n",
    "    def _prepare_data(self):\n",
    "        \"\"\"\n",
    "        Final preparation of data for ML, handling categorical variables\n",
    "        and mixed data types\n",
    "        \"\"\"\n",
    "        # First handle the target variable\n",
    "        target = 'sale_price' if 'sale_price' in self.df.columns else 'SalePrice'\n",
    "        \n",
    "        # Remove rows where target is NaN\n",
    "        print(f\"Initial shape: {self.df.shape}\")\n",
    "        self.df = self.df.dropna(subset=[target])\n",
    "        print(f\"Shape after removing NaN targets: {self.df.shape}\")\n",
    "        \n",
    "        # Convert numeric columns that might have string values\n",
    "        numeric_cols = self.df.select_dtypes(include=['int64', 'float64']).columns\n",
    "        for col in self.df.columns:\n",
    "            # Try to convert to numeric, if fails, leave as is\n",
    "            if col not in numeric_cols:\n",
    "                try:\n",
    "                    self.df[col] = pd.to_numeric(self.df[col], errors='raise')\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # After numeric conversion, get remaining categorical columns\n",
    "        categorical_cols = self.df.select_dtypes(include=['object']).columns\n",
    "        \n",
    "        # Handle categorical variables\n",
    "        for col in categorical_cols:\n",
    "            # Convert all values to strings and handle NaN\n",
    "            self.df[col] = self.df[col].fillna('Missing')\n",
    "            self.df[col] = self.df[col].astype(str)\n",
    "            le = LabelEncoder()\n",
    "            self.df[col] = le.fit_transform(self.df[col])\n",
    "            self.label_encoders[col] = le\n",
    "            \n",
    "        # Remove any remaining unnamed columns\n",
    "        unnamed_cols = [col for col in self.df.columns if 'Unnamed' in col]\n",
    "        self.df = self.df.drop(columns=unnamed_cols)\n",
    "        \n",
    "        # Handle any remaining NaN values in numeric columns\n",
    "        numeric_cols = self.df.select_dtypes(include=['int64', 'float64']).columns\n",
    "        for col in numeric_cols:\n",
    "            if col != target:  # Don't modify target variable\n",
    "                self.df[col] = self.df[col].fillna(self.df[col].median())\n",
    "        \n",
    "        print(\"Data preparation completed successfully\")\n",
    "        \n",
    "    def build_models(self, test_size=0.2, random_state=42):\n",
    "        \"\"\"\n",
    "        Build and evaluate multiple ML models\n",
    "        \"\"\"\n",
    "        # Prepare features and target\n",
    "        target = 'sale_price' if 'sale_price' in self.df.columns else 'SalePrice'\n",
    "        \n",
    "        # Use all columns except target and any problematic ones\n",
    "        exclude_cols = [target] + [col for col in self.df.columns if 'Unnamed' in col]\n",
    "        features = [col for col in self.df.columns if col not in exclude_cols]\n",
    "        \n",
    "        print(f\"Using {len(features)} features for modeling\")\n",
    "        \n",
    "        X = self.df[features]\n",
    "        y = self.df[target]\n",
    "        \n",
    "        # Scale features for models that need it\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        X_scaled = pd.DataFrame(X_scaled, columns=features)\n",
    "        \n",
    "        # Split data\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X_scaled, y, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Split unscaled data for tree-based models\n",
    "        self.X_train_unscaled, self.X_test_unscaled, _, _ = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Initialize models with optimized parameters\n",
    "        self.models = {\n",
    "            'Linear Regression': (LinearRegression(), True),\n",
    "            'Lasso': (Lasso(\n",
    "                alpha=0.01,\n",
    "                random_state=random_state,\n",
    "                max_iter=2000,\n",
    "                tol=0.001,\n",
    "                selection='random',\n",
    "                warm_start=True\n",
    "            ), True),\n",
    "            'Ridge': (Ridge(alpha=0.01, random_state=random_state), True),\n",
    "            'Random Forest': (RandomForestRegressor(\n",
    "                n_estimators=500, \n",
    "                max_depth=20,\n",
    "                min_samples_split=5,\n",
    "                min_samples_leaf=2,\n",
    "                max_features='sqrt',\n",
    "                n_jobs=-1,\n",
    "                random_state=random_state\n",
    "            ), False),\n",
    "            'XGBoost': (xgb.XGBRegressor(\n",
    "                n_estimators=200,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=6,\n",
    "                min_child_weight=1,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                random_state=random_state\n",
    "            ), False),\n",
    "            'Gradient Boosting': (GradientBoostingRegressor(\n",
    "                n_estimators=200,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=6,\n",
    "                min_samples_split=5,\n",
    "                subsample=0.8,\n",
    "                random_state=random_state\n",
    "            ), False),\n",
    "            'Neural Network': (MLPRegressor(\n",
    "                hidden_layer_sizes=(100, 50, 25),\n",
    "                max_iter=1000,\n",
    "                learning_rate='adaptive',\n",
    "                early_stopping=True,\n",
    "                random_state=random_state\n",
    "            ), True)\n",
    "        }\n",
    "        \n",
    "        self.results = {}\n",
    "        self.feature_importance = {}\n",
    "        \n",
    "        for name, (model, use_scaled) in self.models.items():\n",
    "            print(f\"\\nTraining {name}...\")\n",
    "            \n",
    "            # Select appropriate data based on model type\n",
    "            X_train_model = self.X_train if use_scaled else self.X_train_unscaled\n",
    "            X_test_model = self.X_test if use_scaled else self.X_test_unscaled\n",
    "            \n",
    "            # Train model\n",
    "            model.fit(X_train_model, self.y_train)\n",
    "            \n",
    "            # Make predictions\n",
    "            train_pred = model.predict(X_train_model)\n",
    "            test_pred = model.predict(X_test_model)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            self.results[name] = {\n",
    "                'Train R2': r2_score(self.y_train, train_pred),\n",
    "                'Test R2': r2_score(self.y_test, test_pred),\n",
    "                'Train RMSE': np.sqrt(mean_squared_error(self.y_train, train_pred)),\n",
    "                'Test RMSE': np.sqrt(mean_squared_error(self.y_test, test_pred)),\n",
    "                'CV Score': np.mean(cross_val_score(\n",
    "                    model, X_train_model, self.y_train, cv=5, scoring='r2'\n",
    "                ))\n",
    "            }\n",
    "            \n",
    "            # Get feature importance for supported models\n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                importances = pd.DataFrame({\n",
    "                    'feature': features,\n",
    "                    'importance': model.feature_importances_\n",
    "                }).sort_values('importance', ascending=False)\n",
    "                self.feature_importance[name] = importances\n",
    "                \n",
    "            print(f\"{name} completed - Test R2: {self.results[name]['Test R2']:.4f}\")\n",
    "        \n",
    "        # Add ensemble method\n",
    "        self.add_ensemble_method()\n",
    "        \n",
    "        return self.results, self.feature_importance, self.models\n",
    "    \n",
    "    def add_ensemble_method(self):\n",
    "        \"\"\"\n",
    "        Add ensemble method combining the best performing models\n",
    "        \"\"\"\n",
    "        # Select best performing models for ensemble\n",
    "        selected_models = {\n",
    "            name: (model, use_scaled) \n",
    "            for name, (model, use_scaled) in self.models.items()\n",
    "            if name in ['XGBoost', 'Gradient Boosting', 'Ridge']\n",
    "        }\n",
    "        \n",
    "        # Create and fit ensemble\n",
    "        self.ensemble = HousingEnsemble(selected_models)\n",
    "        self.ensemble.fit(self.X_train_unscaled, self.X_train, self.y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        train_pred = self.ensemble.predict(self.X_train_unscaled, self.X_train)\n",
    "        test_pred = self.ensemble.predict(self.X_test_unscaled, self.X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        ensemble_results = {\n",
    "            'Train R2': r2_score(self.y_train, train_pred),\n",
    "            'Test R2': r2_score(self.y_test, test_pred),\n",
    "            'Train RMSE': np.sqrt(mean_squared_error(self.y_train, train_pred)),\n",
    "            'Test RMSE': np.sqrt(mean_squared_error(self.y_test, test_pred)),\n",
    "            'CV Score': np.mean(cross_val_score(\n",
    "                self.models['XGBoost'][0],\n",
    "                self.X_train_unscaled,\n",
    "                self.y_train,\n",
    "                cv=5,\n",
    "                scoring='r2'\n",
    "            ))\n",
    "        }\n",
    "        \n",
    "        # Add ensemble results\n",
    "        self.results['Ensemble'] = ensemble_results\n",
    "        print(\"\\nEnsemble completed - Test R2:\", ensemble_results['Test R2'])\n",
    "    \n",
    "    @staticmethod\n",
    "    def print_results(results, feature_importance):\n",
    "        \"\"\"Print formatted results of model evaluation\"\"\"\n",
    "        print(\"\\nModel Performance Comparison:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        metrics_df = pd.DataFrame(results).T\n",
    "        print(metrics_df.round(4))\n",
    "        \n",
    "        print(\"\\nTop 10 Features by Importance:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for model_name, importance_df in feature_importance.items():\n",
    "            print(f\"\\n{model_name}:\")\n",
    "            print(importance_df.head(10).round(4))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load preprocessed data\n",
    "    file_path = '/Users/jorgemartinez/Documents/NYDSA #3 Machine Learning Project/Ames_Housing_Processed.csv'  # Update with your file path\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Initialize and run ML pipeline\n",
    "    print(\"Initializing ML pipeline...\")\n",
    "    ml_pipeline = AmesHousingML(df)\n",
    "    \n",
    "    print(\"Building models...\")\n",
    "    results, feature_importance, models = ml_pipeline.build_models()\n",
    "    \n",
    "    # Print results\n",
    "    ml_pipeline.print_results(results, feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install \"nbformat>=4.2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install jupyter-dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyter_dash import JupyterDash\n",
    "from dash import dcc, html, Input, Output\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "class DashMLDashboard:\n",
    "    def __init__(self, ml_pipeline):\n",
    "        self.ml_pipeline = ml_pipeline\n",
    "        self.app = JupyterDash(__name__)\n",
    "        self.explainer = shap.TreeExplainer(ml_pipeline.models['XGBoost'][0])\n",
    "        self.shap_values = self.explainer.shap_values(ml_pipeline.X_test)\n",
    "        self.setup_layout()\n",
    "        self.setup_callbacks()\n",
    "        \n",
    "    def setup_layout(self):\n",
    "        self.app.layout = html.Div([\n",
    "            html.H1('ML Model Analysis Dashboard', \n",
    "                   style={'textAlign': 'center', 'padding': '20px', 'color': '#2c3e50'}),\n",
    "            \n",
    "            html.Div([\n",
    "                html.H3('Select View', style={'color': '#34495e'}),\n",
    "                dcc.Dropdown(\n",
    "                    id='view-selector',\n",
    "                    options=[\n",
    "                        {'label': 'Model Performance', 'value': 'performance'},\n",
    "                        {'label': 'Feature Importance', 'value': 'importance'},\n",
    "                        {'label': 'Predictions', 'value': 'predictions'},\n",
    "                        {'label': 'SHAP Analysis', 'value': 'shap'},\n",
    "                        {'label': 'Correlations', 'value': 'correlations'}\n",
    "                    ],\n",
    "                    value='performance',\n",
    "                    style={'marginBottom': '20px'}\n",
    "                ),\n",
    "                \n",
    "                html.Div([\n",
    "                    dcc.Dropdown(\n",
    "                        id='model-selector',\n",
    "                        placeholder='Select Model',\n",
    "                        style={'marginBottom': '20px'}\n",
    "                    ),\n",
    "                ]),\n",
    "                \n",
    "                dcc.Graph(id='main-graph', style={'marginBottom': '20px'}),\n",
    "                dcc.Graph(id='secondary-graph')\n",
    "            ], style={'padding': '20px', 'backgroundColor': '#f8f9fa'})\n",
    "        ])\n",
    "        \n",
    "    def setup_callbacks(self):\n",
    "        @self.app.callback(\n",
    "            [Output('model-selector', 'style'),\n",
    "             Output('model-selector', 'options'),\n",
    "             Output('model-selector', 'value')],\n",
    "            [Input('view-selector', 'value')]\n",
    "        )\n",
    "        def update_model_selector(view):\n",
    "            if view in ['importance', 'predictions']:\n",
    "                models = list(self.ml_pipeline.models.keys()) + ['Ensemble']\n",
    "                return {'display': 'block', 'marginBottom': '20px'}, \\\n",
    "                       [{'label': m, 'value': m} for m in models], \\\n",
    "                       models[0]\n",
    "            return {'display': 'none'}, [], None\n",
    "            \n",
    "        @self.app.callback(\n",
    "            [Output('main-graph', 'figure'),\n",
    "             Output('secondary-graph', 'figure')],\n",
    "            [Input('view-selector', 'value'),\n",
    "             Input('model-selector', 'value')]\n",
    "        )\n",
    "        def update_graphs(view, model):\n",
    "            main_fig = go.Figure()\n",
    "            secondary_fig = go.Figure()\n",
    "            \n",
    "            if view == 'performance':\n",
    "                results_df = pd.DataFrame(self.ml_pipeline.results).T\n",
    "                main_fig = make_subplots(rows=1, cols=2, \n",
    "                                       subplot_titles=('Test R² Scores', 'Test RMSE'))\n",
    "                \n",
    "                main_fig.add_trace(\n",
    "                    go.Bar(x=results_df.index, y=results_df['Test R2'],\n",
    "                          name='R² Score', marker_color='#2ecc71'),\n",
    "                    row=1, col=1\n",
    "                )\n",
    "                main_fig.add_trace(\n",
    "                    go.Bar(x=results_df.index, y=results_df['Test RMSE'],\n",
    "                          name='RMSE', marker_color='#e74c3c'),\n",
    "                    row=1, col=2\n",
    "                )\n",
    "                main_fig.update_layout(\n",
    "                    title='Model Performance Comparison',\n",
    "                    height=500,\n",
    "                    template='plotly_white'\n",
    "                )\n",
    "                \n",
    "            elif view == 'importance' and model:\n",
    "                if model in self.ml_pipeline.feature_importance:\n",
    "                    imp_df = self.ml_pipeline.feature_importance[model]\n",
    "                    top_features = imp_df.head(15)\n",
    "                    main_fig = go.Figure(go.Bar(\n",
    "                        x=top_features['importance'],\n",
    "                        y=top_features['feature'],\n",
    "                        orientation='h',\n",
    "                        marker_color='#3498db'\n",
    "                    ))\n",
    "                    main_fig.update_layout(\n",
    "                        title=f'Top 15 Important Features - {model}',\n",
    "                        xaxis_title='Importance Score',\n",
    "                        yaxis_title='Features',\n",
    "                        height=600,\n",
    "                        template='plotly_white'\n",
    "                    )\n",
    "                    \n",
    "            elif view == 'predictions' and model:\n",
    "                if model == \"Ensemble\":\n",
    "                    predictions = self.ml_pipeline.ensemble.predict(\n",
    "                        self.ml_pipeline.X_test_unscaled,\n",
    "                        self.ml_pipeline.X_test\n",
    "                    )\n",
    "                else:\n",
    "                    model_obj, use_scaled = self.ml_pipeline.models[model]\n",
    "                    X_test = self.ml_pipeline.X_test if use_scaled else self.ml_pipeline.X_test_unscaled\n",
    "                    predictions = model_obj.predict(X_test)\n",
    "                \n",
    "                min_val = min(self.ml_pipeline.y_test.min(), predictions.min())\n",
    "                max_val = max(self.ml_pipeline.y_test.max(), predictions.max())\n",
    "                \n",
    "                main_fig.add_trace(go.Scatter(\n",
    "                    x=self.ml_pipeline.y_test,\n",
    "                    y=predictions,\n",
    "                    mode='markers',\n",
    "                    name='Predictions',\n",
    "                    marker=dict(color='#3498db', size=8)\n",
    "                ))\n",
    "                main_fig.add_trace(go.Scatter(\n",
    "                    x=[min_val, max_val],\n",
    "                    y=[min_val, max_val],\n",
    "                    mode='lines',\n",
    "                    name='Perfect Prediction',\n",
    "                    line=dict(color='#2ecc71', dash='dash')\n",
    "                ))\n",
    "                main_fig.update_layout(\n",
    "                    title=f'Predicted vs Actual Values - {model}',\n",
    "                    xaxis_title='Actual Values',\n",
    "                    yaxis_title='Predicted Values',\n",
    "                    height=600,\n",
    "                    template='plotly_white'\n",
    "                )\n",
    "                \n",
    "            elif view == 'shap':\n",
    "                shap_df = pd.DataFrame(self.shap_values, columns=self.ml_pipeline.X_test.columns)\n",
    "                feature_importance = np.abs(shap_df).mean().sort_values(ascending=False)\n",
    "                \n",
    "                for feature in feature_importance.head(10).index:\n",
    "                    main_fig.add_trace(go.Box(\n",
    "                        y=shap_df[feature],\n",
    "                        name=feature,\n",
    "                        boxpoints='all',\n",
    "                        jitter=0.3,\n",
    "                        pointpos=-1.8\n",
    "                    ))\n",
    "                \n",
    "                main_fig.update_layout(\n",
    "                    title='SHAP Values Distribution for Top 10 Features',\n",
    "                    yaxis_title='SHAP Value',\n",
    "                    height=600,\n",
    "                    template='plotly_white'\n",
    "                )\n",
    "                \n",
    "            elif view == 'correlations':\n",
    "                corr_matrix = self.ml_pipeline.X_train.corr()\n",
    "                main_fig = go.Figure(data=go.Heatmap(\n",
    "                    z=corr_matrix.values,\n",
    "                    x=corr_matrix.columns,\n",
    "                    y=corr_matrix.columns,\n",
    "                    colorscale='RdBu',\n",
    "                    zmid=0\n",
    "                ))\n",
    "                main_fig.update_layout(\n",
    "                    title='Feature Correlation Heatmap',\n",
    "                    height=800,\n",
    "                    template='plotly_white'\n",
    "                )\n",
    "                \n",
    "            return main_fig, secondary_fig\n",
    "            \n",
    "    def run_dashboard(self):\n",
    "        self.app.run_server(mode='external', port=8050, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create the dashboard instance\n",
    "dashboard = DashMLDashboard(ml_pipeline)\n",
    "\n",
    "# Then run it\n",
    "dashboard.run_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Interesting that Lasso and Ridge have the same RMSE on both train and test score, and the same RMSE. I think this is \n",
    "# because you are using an alpha value of 0.001, which is very low. This means that the model is not penalizing the coefficients as much, \n",
    "# and therefore the RMSE is not as high. I would initialize the alpha value to 0.01, and see if that changes the results.\n",
    "\n",
    "# TODO: Also interesting that Random Forest has a higher RMSE than Lasso and Ridge. I would try to increase the number of estimators to 500, \n",
    "# and see if that changes the results.\n",
    "\n",
    "# TODO: XGBOOST seems to have a very high CV score and low RMSE. Did you try submitting the results to the Kaggle competition to see \n",
    "# if it holds against the test set they provided?\n",
    "\n",
    "                  Train R2       Test R2  Train RMSE     Test RMSE  CV Score\n",
    "Linear Regression    0.9745 -1.698148e+21  18318.4304  4.824732e+15    0.8552\n",
    "Lasso                0.9746  9.644000e-01  18272.6180  2.210508e+04    0.9537\n",
    "Ridge                0.9746  9.641000e-01  18270.4963  2.218418e+04    0.9536\n",
    "Random Forest        0.9623  8.961000e-01  22283.6346  3.773712e+04    0.8741\n",
    "XGBoost              0.9999  9.889000e-01   1381.4160  1.233229e+04    0.9904"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
