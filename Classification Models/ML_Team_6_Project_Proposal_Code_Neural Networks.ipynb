{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f884de07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 0 unnamed columns and PID\n",
      "DataFrame shape: (2930, 80)\n",
      "Note: Target creation and splitting moved to Part 2 to prevent data leakage.\n"
     ]
    }
   ],
   "source": [
    "# PART 1: Downloading and Cleaning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"AmesHousing.csv\", index_col=0)\n",
    "\n",
    "# Identify unnamed columns and PID to drop\n",
    "unnamed_cols = [col for col in df.columns if 'unnamed' in col.lower() or 'no meaning' in col.lower()]\n",
    "columns_to_drop = unnamed_cols + ['PID']\n",
    "\n",
    "# Drop identified columns\n",
    "df_cleaned = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "print(f\"Dropped {len(unnamed_cols)} unnamed columns and PID\")\n",
    "print(f\"DataFrame shape: {df_cleaned.shape}\")\n",
    "print(\"Note: Target creation and splitting moved to Part 2 to prevent data leakage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9adca882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before imputation:\n",
      "Pool QC          2917\n",
      "Misc Feature     2824\n",
      "Alley            2732\n",
      "Fence            2358\n",
      "Mas Vnr Type     1775\n",
      "Fireplace Qu     1422\n",
      "Lot Frontage      490\n",
      "Garage Cond       159\n",
      "Garage Yr Blt     159\n",
      "Garage Finish     159\n",
      "dtype: int64\n",
      "------------------------------\n",
      "------------------------------\n",
      "Total missing values remaining in df_sample4: 0\n"
     ]
    }
   ],
   "source": [
    "# Part 2 Missing Value Imputation\n",
    "\n",
    "# Show the number of missing values before we start\n",
    "print(\"Missing values before imputation:\")\n",
    "print(df_cleaned.isnull().sum().sort_values(ascending=False).head(10))\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Drop rows where the dependent variable is missing\n",
    "DV = 'SalePrice'\n",
    "df_sample1 = df_cleaned.dropna(subset=[DV]).copy() # Use .copy() to avoid warnings\n",
    "\n",
    "# --- 1. Impute \"Meaningful NA\" Categoricals ---\n",
    "# These are columns where 'NA' is a category (e.g., \"No Basement\"), not missing data.\n",
    "meaningful_na_columns = [\n",
    "    'Alley', 'Bsmt Qual', 'Bsmt Cond', 'Bsmt Exposure', \n",
    "    'BsmtFin Type 1', 'BsmtFin Type 2', 'FireplaceQu',\n",
    "    'Garage Type', 'Garage Finish', 'Garage Qual', 'Garage Cond',\n",
    "    'Pool QC', 'Fence', 'Misc Feature', 'Mas Vnr Type'\n",
    "]\n",
    "\n",
    "for col in meaningful_na_columns:\n",
    "    if col in df_sample1.columns:\n",
    "        df_sample1[col] = df_sample1[col].fillna('None')\n",
    "\n",
    "# --- 2. Numerical Imputation ---\n",
    "# We create df_sample2 by filling all numerical NAs\n",
    "df_sample2 = df_sample1.copy()\n",
    "\n",
    "# A. Smart Imputation (Context-Aware): Fill with 0\n",
    "# If a house has no basement, its basement-related numericals should be 0, not a median.\n",
    "\n",
    "# Basement-related numericals, if there is no Basement, then the other Basement columns get 0.\n",
    "if 'Bsmt Qual' in df_sample2.columns:\n",
    "    mask = (df_sample2['Bsmt Qual'] == 'None')\n",
    "    bsmt_num_cols = ['Total Bsmt SF', 'BsmtFin SF 1', 'BsmtFin SF 2', 'Bsmt Unf SF', 'Bsmt Full Bath', 'Bsmt Half Bath']\n",
    "    for col in bsmt_num_cols:\n",
    "        if col in df_sample2.columns:\n",
    "            df_sample2.loc[mask, col] = df_sample2.loc[mask, col].fillna(0)\n",
    "\n",
    "# Garage-related numericals: If there is no Garage, then the other Garage columns get 0.\n",
    "if 'Garage Type' in df_sample2.columns:\n",
    "    mask = (df_sample2['Garage Type'] == 'None')\n",
    "    garage_num_cols = ['Garage Cars', 'Garage Area', 'Garage Yr Blt']\n",
    "    for col in garage_num_cols:\n",
    "        if col in df_sample2.columns:\n",
    "            df_sample2.loc[mask, col] = df_sample2.loc[mask, col].fillna(0)\n",
    "\n",
    "# Masonry veneer numericals. If there is no Masonry Veneer Numerical, then the other Masonry columns get 0.\n",
    "if 'Mas Vnr Type' in df_sample2.columns:\n",
    "    mask = (df_sample2['Mas Vnr Type'] == 'None')\n",
    "    if 'Mas Vnr Area' in df_sample2.columns:\n",
    "         df_sample2.loc[mask, 'Mas Vnr Area'] = df_sample2.loc[mask, 'Mas Vnr Area'].fillna(0)\n",
    " \n",
    "# 'Lot Frontage' is likely similar for houses in the same 'Neighborhood', so I use the group median of lot frontage for the neighborhood to impute..\n",
    "if 'Lot Frontage' in df_sample2.columns and 'Neighborhood' in df_sample2.columns:\n",
    "    # Fill NAs with the median Lot Frontage of that specific neighborhood\n",
    "    df_sample2['Lot Frontage'] = df_sample2.groupby('Neighborhood')['Lot Frontage'].transform(lambda x: x.fillna(x.median()))\n",
    "    # If any NAs remain (e.g., a whole neighborhood was NA), fill with the overall median\n",
    "    df_sample2['Lot Frontage'] = df_sample2['Lot Frontage'].fillna(df_sample2['Lot Frontage'].median())\n",
    "\n",
    "# C. Generic Median Imputation (Fallback)\n",
    "# Now, find ALL remaining numerical columns and fill them with their median.\n",
    "# This will handle columns like 'Lot Area' and any NAs our previous logic missed.\n",
    "all_numerical_cols = df_sample2.select_dtypes(include=np.number).columns\n",
    "df_sample2[all_numerical_cols] = df_sample2[all_numerical_cols].fillna(value=df_sample2[all_numerical_cols].median())\n",
    "\n",
    "\n",
    "# --- 3. Categorical Imputation ---\n",
    "# We create df_sample4 by filling all remaining categorical NAs\n",
    "df_sample4 = df_sample2.copy()\n",
    "\n",
    "\n",
    "# B. Generic Mode Imputation (Fallback)\n",
    "# Find ALL remaining categorical/object columns and fill with their mode.\n",
    "all_categorical_cols = df_sample4.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "for col in all_categorical_cols:\n",
    "     df_sample4[col] = df_sample4[col].fillna(df_sample4[col].mode()[0])\n",
    "\n",
    "\n",
    "# --- 4. Final Check ---\n",
    "# This command should now return 0. There should now be no missing values in our dataset.\n",
    "total_missing = df_sample4.isnull().sum().sum()\n",
    "print(\"-\" * 30)\n",
    "print(f\"Total missing values remaining in df_sample4: {total_missing}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f755c33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Median Price: $163,500.00\n",
      "Train shape: (2344, 289)\n",
      "Test shape: (586, 289)\n",
      "Columns are aligned and normalized without leakage.\n"
     ]
    }
   ],
   "source": [
    "# PART 3: Variable Transformation & Splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Define Variable Lists\n",
    "nvar_list_original = [\n",
    "    'Lot Frontage', 'Lot Area', 'Mas Vnr Area', 'BsmtFin SF 1', 'BsmtFin SF 2',\n",
    "    'Bsmt Unf SF', 'Total Bsmt SF', '1st Flr SF', '2nd Flr SF', 'Low Qual Fin SF',\n",
    "    'Gr Liv Area', 'Garage Area', 'Wood Deck SF', 'Open Porch SF', 'Enclosed Porch',\n",
    "    '3Ssn Porch', 'Screen Porch', 'Pool Area', 'Misc Val',\n",
    "    'Overall Qual', 'Overall Cond', 'Year Built', 'Year Remod/Add', 'Bsmt Full Bath',\n",
    "    'Bsmt Half Bath', 'Full Bath', 'Half Bath', 'Bedroom AbvGr', 'Kitchen AbvGr',\n",
    "    'TotRms AbvGrd', 'Fireplaces', 'Garage Yr Blt', 'Garage Cars', 'Mo Sold', 'Yr Sold'\n",
    "]\n",
    "\n",
    "cvar_list_original = [\n",
    "    'MS SubClass', 'MS Zoning', 'Street', 'Alley', 'Land Contour', 'Lot Config',\n",
    "    'Neighborhood', 'Condition 1', 'Condition 2', 'Bldg Type', 'House Style',\n",
    "    'Roof Style', 'Roof Matl', 'Exterior 1st', 'Exterior 2nd', 'Mas Vnr Type',\n",
    "    'Foundation', 'Heating', 'Central Air', 'Garage Type', 'Misc Feature',\n",
    "    'Sale Type', 'Sale Condition', 'Lot Shape', 'Utilities', 'Land Slope', \n",
    "    'Exter Qual', 'Exter Cond', 'Bsmt Qual', 'Bsmt Cond', 'Bsmt Exposure', \n",
    "    'BsmtFin Type 1', 'BsmtFin Type 2', 'Heating QC', 'Electrical', 'Kitchen Qual', \n",
    "    'Functional', 'Fireplace Qu', 'Garage Finish', 'Garage Qual', 'Garage Cond', \n",
    "    'Paved Drive', 'Pool QC', 'Fence'\n",
    "]\n",
    "\n",
    "# 2. Handle Missing Values BEFORE Splitting\n",
    "# Simple fill for demonstration so the math doesn't break\n",
    "df_cleaned[nvar_list_original] = df_cleaned[nvar_list_original].fillna(0)\n",
    "df_cleaned[cvar_list_original] = df_cleaned[cvar_list_original].fillna(\"Missing\")\n",
    "\n",
    "# 3. SPLIT DATA FIRST \n",
    "X = df_cleaned[nvar_list_original + cvar_list_original]\n",
    "y_raw = df_cleaned[\"SalePrice\"]\n",
    "\n",
    "X_train, X_test, y_raw_train, y_raw_test = train_test_split(\n",
    "    X, y_raw, test_size=0.2, random_state=1\n",
    ")\n",
    "\n",
    "# TARGET VARIABLE CREATION \n",
    "# Calculate median ONLY on Train\n",
    "median_price_train = y_raw_train.median()\n",
    "print(f\"Training Median Price: ${median_price_train:,.2f}\")\n",
    "\n",
    "# Apply to both Train and Test\n",
    "y_train = (y_raw_train <= median_price_train).astype(int)\n",
    "y_test = (y_raw_test <= median_price_train).astype(int)\n",
    "\n",
    "#  NUMERICAL STANDARDIZATION  \n",
    "# We create copies \n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "# Calculate Mean and Std ONLY on Train\n",
    "train_means = X_train[nvar_list_original].mean()\n",
    "train_stds = X_train[nvar_list_original].std()\n",
    "\n",
    "# Apply formula to Train\n",
    "X_train_scaled[nvar_list_original] = (X_train[nvar_list_original] - train_means) / train_stds\n",
    "\n",
    "# Apply SAME formula (using Train mean/std) to Test\n",
    "X_test_scaled[nvar_list_original] = (X_test[nvar_list_original] - train_means) / train_stds\n",
    "\n",
    "\n",
    "#  CATEGORICAL DUMMY CODING \n",
    "\n",
    "# 1. Convert to category type\n",
    "X_train_scaled[cvar_list_original] = X_train_scaled[cvar_list_original].astype('category')\n",
    "X_test_scaled[cvar_list_original] = X_test_scaled[cvar_list_original].astype('category')\n",
    "\n",
    "# 2. Get Dummies separately\n",
    "X_train_dummies = pd.get_dummies(X_train_scaled, prefix_sep='_', dtype=int)\n",
    "X_test_dummies = pd.get_dummies(X_test_scaled, prefix_sep='_', dtype=int)\n",
    "\n",
    "# 3. ALIGN COLUMNS\n",
    "# X_test might be missing columns that X_train has (or vice versa).\n",
    "# We enforce X_test to have exactly the same columns as X_train, filling missing ones with 0.\n",
    "X_test_dummies = X_test_dummies.reindex(columns=X_train_dummies.columns, fill_value=0)\n",
    "\n",
    "# 4. Drop Redundant Dummies (The Mode)\n",
    "# We find the mode in TRAIN and drop that specific column from BOTH\n",
    "cols_to_drop = []\n",
    "\n",
    "for var in cvar_list_original:\n",
    "    # Find mode in TRAIN\n",
    "    mode_value = X_train[var].mode()[0] \n",
    "    dummy_col = f\"{var}_{mode_value}\"\n",
    "    \n",
    "    # If this dummy column exists in our dummified dataset, mark it for deletion\n",
    "    if dummy_col in X_train_dummies.columns:\n",
    "        cols_to_drop.append(dummy_col)\n",
    "\n",
    "# Drop the columns from both datasets\n",
    "X_train_final = X_train_dummies.drop(columns=cols_to_drop, errors='ignore')\n",
    "X_test_final = X_test_dummies.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "print(\"Train shape:\", X_train_final.shape)\n",
    "print(\"Test shape:\", X_test_final.shape)\n",
    "print(\"Columns are aligned and normalized without leakage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74af4b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our DV is InBudget_Not in Budget   (1 = Not in Budget, 0 = In Budget)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11afb61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Neural Network Model Summary\n",
      "Section 0. Penalty level alpha: 0.1\n",
      "\n",
      "Section 1. Input (I) -> Hidden (H) Weights (First 5):\n",
      "I:1 -> H:1 - W: 0.0334\n",
      "I:1 -> H:2 - W: 0.3868\n",
      "I:1 -> H:3 - W: -0.2359\n",
      "I:1 -> H:4 - W: 0.0631\n",
      "I:1 -> H:5 - W: 0.0420\n",
      "I:2 -> H:1 - W: -0.6904\n",
      "I:2 -> H:2 - W: 0.2444\n",
      "I:2 -> H:3 - W: -0.4050\n",
      "I:2 -> H:4 - W: -0.1350\n",
      "I:2 -> H:5 - W: -0.8950\n",
      "I:3 -> H:1 - W: -0.0375\n",
      "I:3 -> H:2 - W: 0.0024\n",
      "I:3 -> H:3 - W: -0.2864\n",
      "I:3 -> H:4 - W: -0.0647\n",
      "I:3 -> H:5 - W: -0.7760\n",
      "I:4 -> H:1 - W: -0.7697\n",
      "I:4 -> H:2 - W: -0.1639\n",
      "I:4 -> H:3 - W: -0.3054\n",
      "I:4 -> H:4 - W: -0.2636\n",
      "I:4 -> H:5 - W: -0.2750\n",
      "I:5 -> H:1 - W: 0.1651\n",
      "I:5 -> H:2 - W: 0.0237\n",
      "I:5 -> H:3 - W: 0.6485\n",
      "I:5 -> H:4 - W: 0.0811\n",
      "I:5 -> H:5 - W: 0.2240\n",
      "\n",
      "Section 2. Hidden Node Biases:\n",
      "H:1 - B: 0.0210\n",
      "H:2 - B: 0.8960\n",
      "H:3 - B: 1.2738\n",
      "H:4 - B: -0.3911\n",
      "H:5 - B: 0.6765\n",
      "\n",
      "Starting Grid Search...\n",
      "Best Neural Network Parameters:\n",
      "{'alpha': 1.0, 'hidden_layer_sizes': (5,)}\n",
      "\n",
      "AUC on Test Set: 0.9868\n"
     ]
    }
   ],
   "source": [
    "# PART 5: Neural Network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Initialize parameters\n",
    "alpha = 0.1\n",
    "hidden_layer_sizes = (5,)\n",
    "\n",
    "# Initialize and train model on the processed Training data\n",
    "clf = MLPClassifier(\n",
    "    solver='lbfgs', \n",
    "    alpha=alpha,\n",
    "    hidden_layer_sizes=hidden_layer_sizes, \n",
    "    max_iter=2000,\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "clf.fit(X_train_final, y_train)\n",
    "\n",
    "# Custom summary function\n",
    "def summary_nn(model):\n",
    "    print(\"\\nNeural Network Model Summary\")\n",
    "    print(\"Section 0. Penalty level alpha:\", model.alpha)\n",
    "    \n",
    "    # Only showing first few weights to keep output clean\n",
    "    print(\"\\nSection 1. Input (I) -> Hidden (H) Weights (First 5):\")\n",
    "    if hasattr(model, 'coefs_'):\n",
    "        for i in range(min(5, model.coefs_[0].shape[0])):\n",
    "            for j in range(model.coefs_[0].shape[1]):\n",
    "                print(f\"I:{i+1} -> H:{j+1} - W: {model.coefs_[0][i][j]:.4f}\")\n",
    "    \n",
    "    print(\"\\nSection 2. Hidden Node Biases:\")\n",
    "    if hasattr(model, 'intercepts_'):\n",
    "        for j in range(len(model.intercepts_[0])):\n",
    "            print(f\"H:{j+1} - B: {model.intercepts_[0][j]:.4f}\")\n",
    "\n",
    "summary_nn(clf)\n",
    "\n",
    "# --- GRID SEARCH ---\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(3,), (5,), (10,)],  \n",
    "    'alpha': [0.0001, 0.1, 1.0] \n",
    "}\n",
    "\n",
    "gridsearch = GridSearchCV(\n",
    "    MLPClassifier(solver='lbfgs', max_iter=2000, random_state=1),\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\nStarting Grid Search...\")\n",
    "gridsearch.fit(X_train_final, y_train)\n",
    "clf_best = gridsearch.best_estimator_\n",
    "\n",
    "print(\"Best Neural Network Parameters:\")\n",
    "print(gridsearch.best_params_)\n",
    "\n",
    "# --- EVALUATION ON TEST SET ---\n",
    "# We use the manually processed X_test_final here\n",
    "y_test_proba = clf_best.predict_proba(X_test_final)[:, 1]\n",
    "auc_score = roc_auc_score(y_test, y_test_proba)\n",
    "print(f\"\\nAUC on Test Set: {auc_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
