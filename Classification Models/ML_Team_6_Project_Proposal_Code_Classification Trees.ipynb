{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03c2221b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\jmart0509\\appdata\\local\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\jmart0509\\appdata\\local\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: seaborn in c:\\users\\jmart0509\\appdata\\local\\anaconda3\\lib\\site-packages (0.12.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\jmart0509\\appdata\\local\\anaconda3\\lib\\site-packages (3.8.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jmart0509\\appdata\\local\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Collecting pydotplus\n",
      "  Using cached pydotplus-2.0.2.tar.gz (278 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [14 lines of output]\n",
      "      ERROR: Can not execute `setup.py` since setuptools failed to import in the build environment with exception:\n",
      "      Traceback (most recent call last):\n",
      "        File \"<pip-setuptools-caller>\", line 14, in <module>\n",
      "        File \"c:\\Users\\jmart0509\\AppData\\Local\\anaconda3\\Lib\\site-packages\\setuptools\\__init__.py\", line 26, in <module>\n",
      "          from .dist import Distribution\n",
      "        File \"c:\\Users\\jmart0509\\AppData\\Local\\anaconda3\\Lib\\site-packages\\setuptools\\dist.py\", line 20, in <module>\n",
      "          from . import (\n",
      "        File \"c:\\Users\\jmart0509\\AppData\\Local\\anaconda3\\Lib\\site-packages\\setuptools\\_entry_points.py\", line 6, in <module>\n",
      "          from jaraco.text import yield_lines\n",
      "        File \"c:\\Users\\jmart0509\\AppData\\Local\\anaconda3\\Lib\\site-packages\\setuptools\\_vendor\\jaraco\\text\\__init__.py\", line 12, in <module>\n",
      "          from jaraco.context import ExceptionTrap\n",
      "        File \"c:\\Users\\jmart0509\\AppData\\Local\\anaconda3\\Lib\\site-packages\\setuptools\\_vendor\\jaraco\\context.py\", line 17, in <module>\n",
      "          from backports import tarfile\n",
      "      ImportError: cannot import name 'tarfile' from 'backports' (c:\\Users\\jmart0509\\AppData\\Local\\anaconda3\\Lib\\site-packages\\backports\\__init__.py)\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "%pip install pandas numpy seaborn matplotlib scikit-learn pydotplus ipython joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a458bcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             PID  MS SubClass MS Zoning  Lot Frontage  Lot Area Street Alley  \\\n",
      "Order                                                                          \n",
      "1      526301100           20        RL         141.0     31770   Pave   NaN   \n",
      "2      526350040           20        RH          80.0     11622   Pave   NaN   \n",
      "3      526351010           20        RL          81.0     14267   Pave   NaN   \n",
      "4      526353030           20        RL          93.0     11160   Pave   NaN   \n",
      "5      527105010           60        RL          74.0     13830   Pave   NaN   \n",
      "\n",
      "      Lot Shape Land Contour Utilities  ... Pool Area Pool QC  Fence  \\\n",
      "Order                                   ...                            \n",
      "1           IR1          Lvl    AllPub  ...         0     NaN    NaN   \n",
      "2           Reg          Lvl    AllPub  ...         0     NaN  MnPrv   \n",
      "3           IR1          Lvl    AllPub  ...         0     NaN    NaN   \n",
      "4           Reg          Lvl    AllPub  ...         0     NaN    NaN   \n",
      "5           IR1          Lvl    AllPub  ...         0     NaN  MnPrv   \n",
      "\n",
      "      Misc Feature Misc Val Mo Sold Yr Sold  Sale Type  Sale Condition  \\\n",
      "Order                                                                    \n",
      "1              NaN        0       5    2010        WD           Normal   \n",
      "2              NaN        0       6    2010        WD           Normal   \n",
      "3             Gar2    12500       6    2010        WD           Normal   \n",
      "4              NaN        0       4    2010        WD           Normal   \n",
      "5              NaN        0       3    2010        WD           Normal   \n",
      "\n",
      "       SalePrice  \n",
      "Order             \n",
      "1         215000  \n",
      "2         105000  \n",
      "3         172000  \n",
      "4         244000  \n",
      "5         189900  \n",
      "\n",
      "[5 rows x 81 columns]\n",
      "Dropped 0 unnamed columns and PID\n",
      "DataFrame shape after dropping columns: (2930, 80)\n",
      "       MS SubClass MS Zoning  Lot Frontage  Lot Area Street Alley Lot Shape  \\\n",
      "Order                                                                         \n",
      "1               20        RL         141.0     31770   Pave   NaN       IR1   \n",
      "2               20        RH          80.0     11622   Pave   NaN       Reg   \n",
      "3               20        RL          81.0     14267   Pave   NaN       IR1   \n",
      "4               20        RL          93.0     11160   Pave   NaN       Reg   \n",
      "5               60        RL          74.0     13830   Pave   NaN       IR1   \n",
      "\n",
      "      Land Contour Utilities Lot Config  ... Pool Area Pool QC  Fence  \\\n",
      "Order                                    ...                            \n",
      "1              Lvl    AllPub     Corner  ...         0     NaN    NaN   \n",
      "2              Lvl    AllPub     Inside  ...         0     NaN  MnPrv   \n",
      "3              Lvl    AllPub     Corner  ...         0     NaN    NaN   \n",
      "4              Lvl    AllPub     Corner  ...         0     NaN    NaN   \n",
      "5              Lvl    AllPub     Inside  ...         0     NaN  MnPrv   \n",
      "\n",
      "      Misc Feature Misc Val Mo Sold  Yr Sold  Sale Type  Sale Condition  \\\n",
      "Order                                                                     \n",
      "1              NaN        0       5     2010        WD           Normal   \n",
      "2              NaN        0       6     2010        WD           Normal   \n",
      "3             Gar2    12500       6     2010        WD           Normal   \n",
      "4              NaN        0       4     2010        WD           Normal   \n",
      "5              NaN        0       3     2010        WD           Normal   \n",
      "\n",
      "       SalePrice  \n",
      "Order             \n",
      "1         215000  \n",
      "2         105000  \n",
      "3         172000  \n",
      "4         244000  \n",
      "5         189900  \n",
      "\n",
      "[5 rows x 80 columns]\n"
     ]
    }
   ],
   "source": [
    "# Part 1 Download dataset\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Load data\n",
    "df = pd.read_csv(\"AmesHousing.csv\", index_col=0)\n",
    "\n",
    "#Display first few rows\n",
    "print(df.head())\n",
    "\n",
    "#Identify unnamed columns and PID, which has no meaning\n",
    "unnamed_cols = [col for col in df.columns if 'unnamed' in col.lower() or 'no meaning' in col.lower()]\n",
    "columns_to_drop = unnamed_cols + ['PID']\n",
    "\n",
    "#Drop identified columns\n",
    "df_cleaned = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "print(f\"Dropped {len(unnamed_cols)} unnamed columns and PID\")\n",
    "print(f\"DataFrame shape after dropping columns: {df_cleaned.shape}\")\n",
    "\n",
    "print(df_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b45450a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before imputation:\n",
      "Pool QC          2917\n",
      "Misc Feature     2824\n",
      "Alley            2732\n",
      "Fence            2358\n",
      "Mas Vnr Type     1775\n",
      "Fireplace Qu     1422\n",
      "Lot Frontage      490\n",
      "Garage Cond       159\n",
      "Garage Yr Blt     159\n",
      "Garage Finish     159\n",
      "dtype: int64\n",
      "------------------------------\n",
      "------------------------------\n",
      "Total missing values remaining in df_sample4: 0\n"
     ]
    }
   ],
   "source": [
    "# Part 2 Missing Value Imputation\n",
    "\n",
    "# Show the number of missing values before we start\n",
    "print(\"Missing values before imputation:\")\n",
    "print(df_cleaned.isnull().sum().sort_values(ascending=False).head(10))\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- CHANGE: We CANNOT drop rows where DV is missing, because DV doesn't exist yet.\n",
    "# We proceed using df_cleaned directly.\n",
    "df_sample1 = df_cleaned.copy() \n",
    "\n",
    "# --- 1. Impute \"Meaningful NA\" Categoricals ---\n",
    "# These are columns where 'NA' is a category (e.g., \"No Basement\"), not missing data.\n",
    "meaningful_na_columns = [\n",
    "    'Alley', 'Bsmt Qual', 'Bsmt Cond', 'Bsmt Exposure', \n",
    "    'BsmtFin Type 1', 'BsmtFin Type 2', 'FireplaceQu',\n",
    "    'Garage Type', 'Garage Finish', 'Garage Qual', 'Garage Cond',\n",
    "    'Pool QC', 'Fence', 'Misc Feature', 'Mas Vnr Type'\n",
    "]\n",
    "\n",
    "for col in meaningful_na_columns:\n",
    "    if col in df_sample1.columns:\n",
    "        df_sample1[col] = df_sample1[col].fillna('None')\n",
    "\n",
    "# --- 2. Numerical Imputation ---\n",
    "# We create df_sample2 by filling all numerical NAs\n",
    "df_sample2 = df_sample1.copy()\n",
    "\n",
    "# A. Smart Imputation (Context-Aware): Fill with 0\n",
    "# If a house has no basement, its basement-related numericals should be 0, not a median.\n",
    "\n",
    "# Basement-related numericals, if there is no Basement, then the other Basement columns get 0.\n",
    "if 'Bsmt Qual' in df_sample2.columns:\n",
    "    mask = (df_sample2['Bsmt Qual'] == 'None')\n",
    "    bsmt_num_cols = ['Total Bsmt SF', 'BsmtFin SF 1', 'BsmtFin SF 2', 'Bsmt Unf SF', 'Bsmt Full Bath', 'Bsmt Half Bath']\n",
    "    for col in bsmt_num_cols:\n",
    "        if col in df_sample2.columns:\n",
    "            df_sample2.loc[mask, col] = df_sample2.loc[mask, col].fillna(0)\n",
    "\n",
    "# Garage-related numericals: If there is no Garage, then the other Garage columns get 0.\n",
    "if 'Garage Type' in df_sample2.columns:\n",
    "    mask = (df_sample2['Garage Type'] == 'None')\n",
    "    garage_num_cols = ['Garage Cars', 'Garage Area', 'Garage Yr Blt']\n",
    "    for col in garage_num_cols:\n",
    "        if col in df_sample2.columns:\n",
    "            df_sample2.loc[mask, col] = df_sample2.loc[mask, col].fillna(0)\n",
    "\n",
    "# Masonry veneer numericals. If there is no Masonry Veneer Numerical, then the other Masonry columns get 0.\n",
    "if 'Mas Vnr Type' in df_sample2.columns:\n",
    "    mask = (df_sample2['Mas Vnr Type'] == 'None')\n",
    "    if 'Mas Vnr Area' in df_sample2.columns:\n",
    "         df_sample2.loc[mask, 'Mas Vnr Area'] = df_sample2.loc[mask, 'Mas Vnr Area'].fillna(0)\n",
    " \n",
    "# 'Lot Frontage' is likely similar for houses in the same 'Neighborhood', so I use the group median of lot frontage for the neighborhood to impute..\n",
    "if 'Lot Frontage' in df_sample2.columns and 'Neighborhood' in df_sample2.columns:\n",
    "    # Fill NAs with the median Lot Frontage of that specific neighborhood\n",
    "    df_sample2['Lot Frontage'] = df_sample2.groupby('Neighborhood')['Lot Frontage'].transform(lambda x: x.fillna(x.median()))\n",
    "    # If any NAs remain (e.g., a whole neighborhood was NA), fill with the overall median\n",
    "    df_sample2['Lot Frontage'] = df_sample2['Lot Frontage'].fillna(df_sample2['Lot Frontage'].median())\n",
    "\n",
    "# C. Generic Median Imputation (Fallback)\n",
    "# Now, find ALL remaining numerical columns and fill them with their median.\n",
    "# This will handle columns like 'Lot Area' and any NAs our previous logic missed.\n",
    "all_numerical_cols = df_sample2.select_dtypes(include=np.number).columns\n",
    "df_sample2[all_numerical_cols] = df_sample2[all_numerical_cols].fillna(value=df_sample2[all_numerical_cols].median())\n",
    "\n",
    "\n",
    "# --- 3. Categorical Imputation ---\n",
    "# We create df_sample4 by filling all remaining categorical NAs\n",
    "df_sample4 = df_sample2.copy()\n",
    "\n",
    "\n",
    "# B. Generic Mode Imputation (Fallback)\n",
    "# Find ALL remaining categorical/object columns and fill with their mode.\n",
    "all_categorical_cols = df_sample4.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "for col in all_categorical_cols:\n",
    "     df_sample4[col] = df_sample4[col].fillna(df_sample4[col].mode()[0])\n",
    "\n",
    "\n",
    "# --- 4. Final Check ---\n",
    "# This command should now return 0. There should now be no missing values in our dataset.\n",
    "total_missing = df_sample4.isnull().sum().sum()\n",
    "print(\"-\" * 30)\n",
    "print(f\"Total missing values remaining in df_sample4: {total_missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96c00425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared. Columns including SalePrice: 289\n"
     ]
    }
   ],
   "source": [
    "# Part 3 Variable Transformation\n",
    "\n",
    "# 1) Define variable lists\n",
    "nvar_list = [\n",
    "    'Lot Frontage', 'Lot Area', 'Mas Vnr Area', 'BsmtFin SF 1', 'BsmtFin SF 2',\n",
    "    'Bsmt Unf SF', 'Total Bsmt SF', '1st Flr SF', '2nd Flr SF', 'Low Qual Fin SF',\n",
    "    'Gr Liv Area', 'Garage Area', 'Wood Deck SF', 'Open Porch SF', 'Enclosed Porch',\n",
    "    '3Ssn Porch', 'Screen Porch', 'Pool Area', 'Misc Val',\n",
    "    'Overall Qual', 'Overall Cond', 'Year Built', 'Year Remod/Add', 'Bsmt Full Bath',\n",
    "    'Bsmt Half Bath', 'Full Bath', 'Half Bath', 'Bedroom AbvGr', 'Kitchen AbvGr',\n",
    "    'TotRms AbvGrd', 'Fireplaces', 'Garage Yr Blt', 'Garage Cars', 'Mo Sold', 'Yr Sold'\n",
    "]\n",
    "\n",
    "cvar_list = [\n",
    "    'MS SubClass', 'MS Zoning', 'Street', 'Alley', 'Land Contour', 'Lot Config',\n",
    "    'Neighborhood', 'Condition 1', 'Condition 2', 'Bldg Type', 'House Style',\n",
    "    'Roof Style', 'Roof Matl', 'Exterior 1st', 'Exterior 2nd', 'Mas Vnr Type',\n",
    "    'Foundation', 'Heating', 'Central Air', 'Garage Type', 'Misc Feature',\n",
    "    'Sale Type', 'Sale Condition', \n",
    "    'Lot Shape', 'Utilities', 'Land Slope', 'Exter Qual', 'Exter Cond',\n",
    "    'Bsmt Qual', 'Bsmt Cond', 'Bsmt Exposure', 'BsmtFin Type 1', 'BsmtFin Type 2',\n",
    "    'Heating QC', 'Electrical', 'Kitchen Qual', 'Functional', 'Fireplace Qu',\n",
    "    'Garage Finish', 'Garage Qual', 'Garage Cond', 'Paved Drive', 'Pool QC', 'Fence'\n",
    "]\n",
    "\n",
    "\n",
    "# 2) Make a modeling df that includes predictors.\n",
    "needed_cols = nvar_list + cvar_list + ['SalePrice']\n",
    "df_model = df_sample4[needed_cols].copy()\n",
    "\n",
    "# 4) Type-cast\n",
    "# SalePrice is numerical, but not in nvar_list, so it remains float/int automatically.\n",
    "df_model[cvar_list] = df_model[cvar_list].astype('category')\n",
    "df_model[nvar_list] = df_model[nvar_list].astype('float64')\n",
    "\n",
    "# 5) Dummy-code ONLY predictor categoricals\n",
    "X_num = df_model[nvar_list]\n",
    "X_cat = pd.get_dummies(df_model[cvar_list], prefix_sep='_', dtype=int)\n",
    "\n",
    "# Drop one baseline dummy PER predictor variable\n",
    "for var in cvar_list:\n",
    "    # drop the most frequent category’s dummy for each predictor\n",
    "    mode_val = df_sample4[var].mode()[0]\n",
    "    colname = f\"{var}_{mode_val}\"\n",
    "    if colname in X_cat.columns:\n",
    "        X_cat = X_cat.drop(columns=[colname])\n",
    "\n",
    "# Combine predictors; \n",
    "X_full = pd.concat([X_num, X_cat, df_model[['SalePrice']]], axis=1)\n",
    "\n",
    "# We do not define 'y' here yet.\n",
    "print(f\"Data prepared. Columns including SalePrice: {X_full.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17850747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Feature Count: 289\n",
      "Filtered Feature Count (including SalePrice): 60\n",
      "Training Median Sale Price: $163,500\n",
      "\n",
      "Partition Complete.\n",
      "Training Data X: (2344, 59)\n",
      "Testing Data X: (586, 59)\n",
      "Training Data y: (2344,)\n"
     ]
    }
   ],
   "source": [
    "# PART 4: Feature Selection and Partition as we do not use every variable in the dataset for our model \n",
    "\n",
    "# 1. Define the specific features you want to use\n",
    "my_nvar_list = [\n",
    "    'Lot Area', 'Overall Qual', 'Year Built', 'Total Bsmt SF', 'Gr Liv Area',\n",
    "    'Full Bath', 'Garage Cars', 'Mas Vnr Area', '1st Flr SF'\n",
    "]\n",
    "\n",
    "my_cvar_list = [\n",
    "    'Neighborhood_Blmngtn', 'Neighborhood_Blueste', 'Neighborhood_BrDale',\n",
    "    'Neighborhood_BrkSide', 'Neighborhood_ClearCr', 'Neighborhood_CollgCr',\n",
    "    'Neighborhood_Crawfor', 'Neighborhood_Edwards', 'Neighborhood_Gilbert',\n",
    "    'Neighborhood_Greens', 'Neighborhood_GrnHill', 'Neighborhood_IDOTRR',\n",
    "    'Neighborhood_Landmrk', 'Neighborhood_MeadowV', 'Neighborhood_Mitchel',\n",
    "    'Neighborhood_NPkVill', 'Neighborhood_NWAmes', 'Neighborhood_NoRidge',\n",
    "    'Neighborhood_NridgHt', 'Neighborhood_OldTown', 'Neighborhood_SWISU',\n",
    "    'Neighborhood_Sawyer', 'Neighborhood_SawyerW', 'Neighborhood_Somerst',\n",
    "    'Neighborhood_StoneBr', 'Neighborhood_Timber', 'Neighborhood_Veenker',\n",
    "    'House Style_1.5Fin', 'House Style_1.5Unf', 'House Style_2.5Fin',\n",
    "    'House Style_2.5Unf', 'House Style_2Story', 'House Style_SFoyer',\n",
    "    'House Style_SLvl', 'Bldg Type_2fmCon', 'Bldg Type_Duplex',\n",
    "    'Bldg Type_Twnhs', 'Bldg Type_TwnhsE', 'Kitchen Qual_Ex', 'Kitchen Qual_Fa',\n",
    "    'Kitchen Qual_Gd', 'Kitchen Qual_Po', 'Exter Qual_Ex', 'Exter Qual_Fa',\n",
    "    'Exter Qual_Gd', 'Foundation_BrkTil', 'Foundation_CBlock', 'Foundation_Slab',\n",
    "    'Foundation_Stone', 'Foundation_Wood' \n",
    "]\n",
    "\n",
    "# 2. Filter your 'X' from Part 3 to include ONLY these columns PLUS SalePrice\n",
    "desired_columns = my_nvar_list + my_cvar_list + ['SalePrice']\n",
    "valid_columns = X_full.columns.intersection(desired_columns)\n",
    "\n",
    "print(f\"Original Feature Count: {X_full.shape[1]}\")\n",
    "X_subset = X_full[valid_columns]\n",
    "print(f\"Filtered Feature Count (including SalePrice): {X_subset.shape[1]}\")\n",
    "\n",
    "# 3. Partition the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# We split the dataset FIRST\n",
    "train_df, test_df = train_test_split(\n",
    "    X_subset, test_size=0.2, random_state=1\n",
    ")\n",
    "\n",
    "# --- 4. CREATE DV USING TRAINING MEDIAN ---\n",
    "\n",
    "# Calculate Median on Training Data Only\n",
    "train_median = train_df['SalePrice'].median()\n",
    "print(f\"Training Median Sale Price: ${train_median:,.0f}\")\n",
    "\n",
    "# Create 'DV' column: 1 if In Budget (<= Median), 0 if Not\n",
    "# Applying this logic to both Train and Test using train_median\n",
    "y_train = (train_df['SalePrice'] <= train_median).astype(int)\n",
    "y_test = (test_df['SalePrice'] <= train_median).astype(int)\n",
    "\n",
    "# Remove 'SalePrice' from the predictors so the model doesn't cheat\n",
    "X_train = train_df.drop(columns=['SalePrice'])\n",
    "X_test = test_df.drop(columns=['SalePrice'])\n",
    "\n",
    "print(\"\\nPartition Complete.\")\n",
    "print(f\"Training Data X: {X_train.shape}\")\n",
    "print(f\"Testing Data X: {X_test.shape}\")\n",
    "print(f\"Training Data y: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c513b38",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydotplus'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m roc_auc_score\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringIO\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpydotplus\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pydotplus'"
     ]
    }
   ],
   "source": [
    "# PART 5: Classification Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from io import StringIO\n",
    "import pydotplus\n",
    "from IPython.display import Image\n",
    "import numpy as np\n",
    "\n",
    "# 1. User-Defined Function: Summary Tree (Visualization)\n",
    "\n",
    "def summary_tree(model_object):\n",
    "  # Note: We use X_train.columns.values to get feature names\n",
    "  dot_data = StringIO()\n",
    "  export_graphviz(model_object, out_file=dot_data, filled=True,\n",
    "                  rounded=True, special_characters=True, \n",
    "                  feature_names=X_train.columns.values,\n",
    "                  class_names=['0', '1'])\n",
    "  graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "  output_imagefile = 'tree.png'\n",
    "  graph.write_png(output_imagefile)\n",
    "  return output_imagefile\n",
    "\n",
    "\n",
    "# 3. Run Classification Tree with GridSearchCV\n",
    "\n",
    "kfolds = 5\n",
    "maximum_depth = 100\n",
    "minimum_depth = 1\n",
    "param_grid = {'max_depth': list(range(minimum_depth, maximum_depth+1))}\n",
    "\n",
    "\n",
    "gridsearch = GridSearchCV(\n",
    "    DecisionTreeClassifier(criterion='entropy', random_state=1), \n",
    "    param_grid, \n",
    "    scoring='roc_auc', \n",
    "    cv=kfolds, \n",
    "    n_jobs=1 \n",
    ")\n",
    "\n",
    "# Fit the model using the training data \n",
    "gridsearch.fit(X_train, y_train)\n",
    "clf_BPT = gridsearch.best_estimator_\n",
    "\n",
    "# 4. Display the Tree and Stats\n",
    "\n",
    "# Display the best pruned tree image\n",
    "display(Image(summary_tree(clf_BPT)))\n",
    "\n",
    "# Display depth and AUC\n",
    "print(f\"Best Depth: {clf_BPT.get_depth()}\")\n",
    "print(f\"Test AUC: {roc_auc_score(y_test, clf_BPT.predict_proba(X_test)[:,1])}\")\n",
    "\n",
    "\n",
    "# 5. Leaf Node Statistics\n",
    "# Modified by Reid Lapekas during Nov. 2024\n",
    "\n",
    "def get_treepaths(dtc, df):\n",
    "    rules_list = []\n",
    "    values_path = []\n",
    "    values = dtc.tree_.value\n",
    "\n",
    "    def RevTraverseTree(tree, node, rules, pathValues):\n",
    "        try:\n",
    "            prevnode = tree[2].index(node)\n",
    "            leftright = '<='\n",
    "            pathValues.append(values[prevnode])\n",
    "        except ValueError:\n",
    "            # failed, so find it as a right node\n",
    "            prevnode = tree[3].index(node)\n",
    "            leftright = '>'\n",
    "            pathValues.append(values[prevnode])\n",
    "\n",
    "        # Get the rule\n",
    "        p1 = df.columns[tree[0][prevnode]]\n",
    "        p2 = tree[1][prevnode]\n",
    "        rules.append(str(p1) + ' ' + leftright + ' ' + str(p2))\n",
    "\n",
    "        # If not at top, go up one step\n",
    "        if prevnode != 0:\n",
    "            RevTraverseTree(tree, prevnode, rules, pathValues)\n",
    "\n",
    "    # Get leaf nodes\n",
    "    leaves = dtc.tree_.children_left == -1\n",
    "    leaves = np.arange(0,dtc.tree_.node_count)[leaves]\n",
    "\n",
    "    # Build simplified tree structure\n",
    "    thistree = [dtc.tree_.feature.tolist()]\n",
    "    thistree.append(dtc.tree_.threshold.tolist())\n",
    "    thistree.append(dtc.tree_.children_left.tolist())\n",
    "    thistree.append(dtc.tree_.children_right.tolist())\n",
    "\n",
    "    # Apply rules\n",
    "    for (ind,nod) in enumerate(leaves):\n",
    "        rules = []\n",
    "        pathValues = []\n",
    "        RevTraverseTree(thistree, nod, rules, pathValues)\n",
    "\n",
    "        pathValues.insert(0, values[nod])\n",
    "        pathValues = list(reversed(pathValues))\n",
    "        rules = list(reversed(rules))\n",
    "        rules_list.append(rules)\n",
    "        values_path.append(pathValues)\n",
    "\n",
    "    # Print results\n",
    "    for i in range(len(rules_list)):\n",
    "      print('\\nLeaf node ID =', i+1)\n",
    "      print('Path =', rules_list[i])\n",
    "      samples = dtc.tree_.n_node_samples[leaves[i]]\n",
    "      # Probability * Samples = Count\n",
    "      class_counts = np.round(values_path[i][-1][0] * samples).astype(int)\n",
    "      \n",
    "      # Note: Because of how sklearn stores values, we normalize to be safe\n",
    "      # (Sometimes values_path contains raw counts, sometimes weighted)\n",
    "      current_val = values_path[i][-1][0]\n",
    "      if np.sum(current_val) > 0:\n",
    "          normalized_probs = current_val / np.sum(current_val)\n",
    "          class_counts = np.round(normalized_probs * samples).astype(int)\n",
    "      \n",
    "      print('sample =', int(samples))\n",
    "      print('value =', list(class_counts))\n",
    "      \n",
    "      predicted_class = np.argmax(class_counts)\n",
    "      print('class = ', predicted_class)\n",
    "\n",
    "    return None\n",
    "\n",
    "# Run the function on the Training data\n",
    "get_treepaths(dtc=clf_BPT, df=X_train)\n",
    "\n",
    "\n",
    "# Part 6: English Rule Descriptions\n",
    "\n",
    "\n",
    "def extract_english_rules(tree_model, feature_names, class_labels=['0', '1']):\n",
    "    tree_ = tree_model.tree_\n",
    "    feature_name = [feature_names[i] if i != -2 else \"undefined!\" for i in tree_.feature]\n",
    "    paths = []\n",
    "    path = []\n",
    "\n",
    "    def recurse(node, path, paths):\n",
    "        if tree_.feature[node] != -2:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            recurse(tree_.children_left[node], path + [f\"{name} <= {threshold:.2f}\"], paths)\n",
    "            recurse(tree_.children_right[node], path + [f\"{name} > {threshold:.2f}\"], paths)\n",
    "        else:\n",
    "            value = tree_.value[node][0]\n",
    "            n_samples = int(np.sum(value))\n",
    "            class_counts = [int(v) for v in value]\n",
    "            p1 = class_counts[1] / n_samples if n_samples > 0 else 0\n",
    "            paths.append({\n",
    "                \"rule\": \" AND \".join(path),\n",
    "                \"samples\": n_samples,\n",
    "                \"class_0\": class_counts[0],\n",
    "                \"class_1\": class_counts[1],\n",
    "                \"predicted_probability\": round(p1, 3),\n",
    "                \"predicted_class\": class_labels[int(np.argmax(value))]\n",
    "            })\n",
    "\n",
    "    recurse(0, path, paths)\n",
    "    return pd.DataFrame(paths).sort_values(by=[\"predicted_probability\"], ascending=False)\n",
    "\n",
    "# Generate and save\n",
    "rules_df = extract_english_rules(clf_BPT, X_train.columns.values)\n",
    "print(\"\\nTop 5 Rules:\")\n",
    "display(rules_df.head(5))\n",
    "rules_df.to_csv(\"tree_rules.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
